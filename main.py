#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»ç¨‹å¼ï¼šOllama æ¨¡å‹è©•æ¯”å·¥å…·

æœ¬ç¨‹å¼æœƒè‡ªå‹•åŒ–åŸ·è¡Œä¸€ç³»åˆ—é‡å°æœ¬æ©Ÿ Ollama æ¨¡å‹çš„è©•æ¯”ä»»å‹™ï¼Œ
åŒ…æ‹¬ç¿»è­¯å’Œæ‘˜è¦ã€‚å®ƒæœƒä½¿ç”¨è¨­å®šå¥½çš„é›²ç«¯å¤§å‹èªè¨€æ¨¡å‹ï¼ˆå¦‚ GPT-4ã€Geminiï¼‰
ä½œç‚ºè©•å¯©ï¼Œå°å„å€‹ Ollama æ¨¡å‹çš„è¡¨ç¾é€²è¡Œè©•åˆ†ï¼Œä¸¦æœ€çµ‚ç”ŸæˆåŒ…å«
è¡¨æ ¼ã€çµ±è¨ˆåˆ†æèˆ‡è¦–è¦ºåŒ–åœ–è¡¨çš„ Markdown å’Œ HTML å ±è¡¨ã€‚

ä¸»è¦æµç¨‹ï¼š
1. è®€å–è¨­å®šæª” `config.py` ä¸­çš„æ¨¡å‹æ¸…å–®èˆ‡ API é‡‘é‘°ã€‚
2. è®€å– `input.txt` ä½œç‚ºæ‰€æœ‰ä»»å‹™çš„è¼¸å…¥æ–‡æœ¬ã€‚
3. ä¾åºå‘¼å« `OLLAMA_MODELS_TO_COMPARE` ä¸­çš„æ¯å€‹æ¨¡å‹ï¼ŒåŸ·è¡Œ `SUPPORTED_TASKS` ä¸­å®šç¾©çš„ä»»å‹™ã€‚
4. å°‡æ¯å€‹æ¨¡å‹çš„è¼¸å‡ºçµæœï¼Œäº¤ç”± `REVIEWER_MODELS` ä¸­å®šç¾©çš„è©•å¯©æ¨¡å‹é€²è¡Œè©•åˆ†ã€‚
5. å½™ç¸½æ‰€æœ‰è©•åˆ†èˆ‡çµæœï¼Œç”Ÿæˆ Markdown æ ¼å¼çš„å ±è¡¨ã€‚
6. å°‡ Markdown å ±è¡¨è½‰æ›ç‚º HTML æ ¼å¼ï¼Œæ–¹ä¾¿ç€è¦½ã€‚
7. æ‰€æœ‰ API å‘¼å«çµæœéƒ½æœƒè¢«å¿«å–ï¼Œé¿å…é‡è¤‡åŸ·è¡Œæµªè²»æ™‚é–“èˆ‡è³‡æºã€‚
"""

import re
import sys
import time
from openai import OpenAI
import requests
from typing import Tuple
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

# å¾è¨­å®šæª”å°å…¥æ‰€æœ‰å¿…è¦çš„åƒæ•¸
from config import (
    OLLAMA_API_BASE_URL,
    OLLAMA_MODELS_TO_COMPARE,
    OPENAI_API_KEY,
    GOOGLE_API_KEY,
    OPENROUTER_API_KEY,
    REPLICATE_API_KEY,
    REVIEWER_MODELS,
    REVIEWER_TEMPERATURE,
    SUPPORTED_TASKS,
)
# å¾å·¥å…·æ¨¡çµ„å°å…¥ HTML è½‰æ›å™¨èˆ‡å¿«å–å·¥å…·
from markdown2html import convert_markdown_to_html
from cache_utils import get_cache_key, load_from_cache, save_to_cache

# --- å…¨åŸŸè¨­å®š ---
# è¨­å®š Matplotlib ä½¿ç”¨çš„å­—é«”ï¼Œä»¥ç¢ºä¿åœ–è¡¨ä¸­çš„ä¸­æ–‡èƒ½æ­£å¸¸é¡¯ç¤ºã€‚
# Arial Unicode MS å’Œ SimHei æ˜¯å¸¸ç”¨çš„ä¸­æ–‡å­—é«”ã€‚
plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei", "DejaVu Sans"]
# è§£æ±º Matplotlib åœ–è¡¨ä¸­çš„è² è™Ÿé¡¯ç¤ºå•é¡Œã€‚
plt.rcParams["axes.unicode_minus"] = False


class ModelEvaluator:
    """
    æ¨¡å‹è©•æ¯”å™¨é¡åˆ¥ï¼Œå°è£äº†æ‰€æœ‰è©•æ¯”ç›¸é—œçš„é‚è¼¯ã€‚

    é€™å€‹é¡åˆ¥è² è²¬ç®¡ç†æ•´å€‹è©•æ¯”æµç¨‹ï¼Œå¾è®€å–è¼¸å…¥ã€å‘¼å«æ¨¡å‹ã€
    é€²è¡Œè©•å¯©ï¼Œåˆ°æœ€çµ‚ç”Ÿæˆå ±è¡¨ã€‚
    """

    def __init__(self):
        """
        åˆå§‹åŒ–è©•æ¯”å™¨ã€‚

        - `self.results`: ä¸€å€‹å­—å…¸ï¼Œç”¨ä¾†å„²å­˜æ¯å€‹ Ollama æ¨¡å‹åœ¨å„é …ä»»å‹™çš„åŸå§‹è¼¸å‡ºçµæœã€‚
                          çµæ§‹ï¼š{ "æ¨¡å‹åç¨±": { "ä»»å‹™åç¨±": "è¼¸å‡ºæ–‡å­—" } }
        - `self.evaluation_scores`: ä¸€å€‹å­—å…¸ï¼Œç”¨ä¾†å„²å­˜æ¯å€‹è©•å¯©æ¨¡å‹å° Ollama æ¨¡å‹è¼¸å‡ºçµæœçš„è©•åˆ†ã€‚
                                     çµæ§‹ï¼š{ "è©•å¯©è€…ID": { "æ¨¡å‹åç¨±": { "ä»»å‹™åç¨±": { "score": åˆ†æ•¸, "comment": "è©•èª" } } } }
        """
        self.results = {}
        self.evaluation_scores = {}

    def read_input_text(self, file_path: str = "input.txt") -> str:
        """
        è®€å–æŒ‡å®šçš„æ¸¬è©¦æ¨£æœ¬æ–‡ä»¶ã€‚

        Args:
            file_path (str): è¼¸å…¥æ–‡ä»¶çš„è·¯å¾‘ï¼Œé è¨­ç‚º "input.txt"ã€‚

        Returns:
            str: æ–‡ä»¶çš„å®Œæ•´å…§å®¹ã€‚

        Raises:
            SystemExit: å¦‚æœæ–‡ä»¶æ‰¾ä¸åˆ°æˆ–è®€å–æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œå‰‡æœƒå°å‡ºéŒ¯èª¤è¨Šæ¯ä¸¦çµ‚æ­¢ç¨‹å¼ã€‚
        """
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {file_path}")
            sys.exit(1)  # çµ‚æ­¢ç¨‹å¼
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            sys.exit(1)  # çµ‚æ­¢ç¨‹å¼

    def call_ollama_api(self, model: str, task: str, text: str) -> str:
        """
        å‘¼å«æœ¬æ©Ÿçš„ Ollama API ä¾†åŸ·è¡ŒæŒ‡å®šä»»å‹™ã€‚

        åœ¨å‘¼å«å‰æœƒå…ˆæª¢æŸ¥å¿«å–ï¼Œå¦‚æœå¿«å–ä¸­å·²æœ‰ç›¸åŒè«‹æ±‚çš„çµæœï¼Œå‰‡ç›´æ¥è¿”å›å¿«å–çµæœã€‚
        å¦å‰‡ï¼Œæœƒå‘ Ollama ç™¼é€è«‹æ±‚ï¼Œä¸¦å°‡æ–°çµæœå­˜å…¥å¿«å–ã€‚

        Args:
            model (str): è¦ä½¿ç”¨çš„ Ollama æ¨¡å‹åç¨±ã€‚
            task (str): è¦åŸ·è¡Œçš„ä»»å‹™åç¨±ï¼ˆä¾‹å¦‚ "translate" æˆ– "summarize"ï¼‰ã€‚
            text (str): è¼¸å…¥çµ¦æ¨¡å‹çš„æ–‡å­—ã€‚

        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„æ–‡å­—çµæœã€‚å¦‚æœç™¼ç”ŸéŒ¯èª¤ï¼Œå‰‡è¿”å›ä»¥ "ERROR:" é–‹é ­çš„éŒ¯èª¤è¨Šæ¯ã€‚
        """
        # å»ºç«‹å¿«å–åƒæ•¸å­—å…¸èˆ‡å¿«å–éµ
        # å¾è¨­å®šæª”ä¸­å–å¾—è©²ä»»å‹™å°æ‡‰çš„ç³»çµ±æç¤ºè©
        prompt: str = SUPPORTED_TASKS[task]

        # å»ºç«‹å¿«å–åƒæ•¸å­—å…¸èˆ‡å¿«å–éµ
        cache_params = {
            "provider": "ollama",
            "model": model,
            "task": task,
            "text": text,
        }
        cache_key = get_cache_key(cache_params, prompt=prompt)
        # å˜—è©¦å¾å¿«å–è¼‰å…¥
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… {model} ({task}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        # è¨­å®š API ç«¯é»èˆ‡æ¨™é ­
        url: np.LiteralString = f"{OLLAMA_API_BASE_URL}/api/chat"
        headers: dict[str, str] = {"Content-Type": "application/json"}

        # å¾è¨­å®šæª”ä¸­å–å¾—è©²ä»»å‹™å°æ‡‰çš„ç³»çµ±æç¤ºè©
        prompt: str = SUPPORTED_TASKS[task]

        # æº–å‚™è«‹æ±‚çš„è³‡æ–™çµæ§‹
        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": prompt},
                {"role": "user", "content": text},
            ],
            "stream": False,  # é—œé–‰ä¸²æµè¼¸å‡ºï¼Œä¸€æ¬¡æ€§å–å¾—å®Œæ•´çµæœ
        }

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« {model} åŸ·è¡Œ {task} ä»»å‹™...")
            # ç™¼é€ POST è«‹æ±‚ï¼Œè¨­å®šè¼ƒé•·çš„è¶…æ™‚æ™‚é–“ï¼ˆ500ç§’ï¼‰ï¼Œå› ç‚ºæœ¬æ©Ÿæ¨¡å‹å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“å›æ‡‰
            response: requests.Response = requests.post(
                url, headers=headers, json=data, timeout=500
            )
            response.raise_for_status()  # å¦‚æœ HTTP ç‹€æ…‹ç¢¼æ˜¯ 4xx æˆ– 5xxï¼Œå‰‡æ‹‹å‡ºç•°å¸¸

            # è§£æ JSON å›æ‡‰
            result = response.json()
            output_text: str = result["message"]["content"].strip()
            # ç§»é™¤æ¨¡å‹å›æ‡‰ä¸­å¯èƒ½åŒ…å«çš„ <think>...</think> æ¨™ç±¤ï¼ˆæŸäº›æ¨¡å‹æœƒç”¨ä¾†è¡¨ç¤ºæ€è€ƒéç¨‹ï¼‰
            output_text: str = re.sub(
                r"<think>.*?</think>", "", output_text, flags=re.DOTALL
            )
            # å°‡æˆåŠŸå–å¾—çš„çµæœå­˜å…¥å¿«å–
            save_to_cache(cache_key, output_text)
            return output_text

        except requests.exceptions.Timeout:
            print(f"  âš ï¸  {model} å›æ‡‰è¶…æ™‚")
            return "ERROR: å›æ‡‰è¶…æ™‚"
        except requests.exceptions.RequestException as e:
            print(f"  âŒ {model} API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: API å‘¼å«å¤±æ•— - {e}"
        except Exception as e:
            print(f"  âŒ {model} è™•ç†å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ERROR: è™•ç†å›æ‡‰å¤±æ•— - {e}"

    def call_openai_api(
        self,
        model: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """
        å‘¼å« OpenAI APIã€‚

        åŒæ¨£æ”¯æ´å¿«å–æ©Ÿåˆ¶ã€‚æœƒæ ¹æ“šæ¨¡å‹åç¨±å¾è¨­å®šæª”ä¸­å–å¾—å°æ‡‰çš„ `temperature` åƒæ•¸ã€‚

        Args:
            model (str): è¦ä½¿ç”¨çš„ OpenAI æ¨¡å‹åç¨± (e.g., "gpt-4.1-mini")ã€‚
            system_prompt (str): ç³»çµ±æç¤ºè©ã€‚
            user_content (str): ä½¿ç”¨è€…è¼¸å…¥çš„å…§å®¹ã€‚
            is_reviewer (bool): æ¨™è¨˜æ­¤å‘¼å«æ˜¯å¦ç‚ºè©•å¯©ç”¨é€”ï¼Œä¸»è¦ç”¨æ–¼å¿«å–éµçš„å€åˆ†ã€‚

        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„æ–‡å­—çµæœæˆ–éŒ¯èª¤è¨Šæ¯ã€‚
        """
        # æª¢æŸ¥å¿«å–
        full_prompt = f"{system_prompt}\n\n{user_content}"
        cache_params = {
            "provider": "openai",
            "model": model,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params, prompt=full_prompt)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… OpenAI ({model}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        # æª¢æŸ¥ API é‡‘é‘°æ˜¯å¦å·²è¨­å®š
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
            return "ERROR: OpenAI API é‡‘é‘°æœªè¨­å®š"

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« OpenAI API ({model})...")

            # å¾è¨­å®šæª”ä¸­å–å¾—è©²è©•å¯©æ¨¡å‹çš„ temperature
            temperature = REVIEWER_TEMPERATURE.get(model, 0.1)

            # å»ºç«‹è«‹æ±‚åƒæ•¸å­—å…¸
            request_params = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content},
                ],
                "timeout": 60,  # è¨­å®š 60 ç§’è¶…æ™‚
            }

            # æŸäº›æ¨¡å‹ï¼ˆå¦‚ gpt-4o-miniï¼‰ä¸æ”¯æ´ temperature=1ï¼Œæ‰€ä»¥åªæœ‰åœ¨ä¸ç‚º None æˆ– 1 æ™‚æ‰åŠ å…¥æ­¤åƒæ•¸
            if temperature is not None and temperature != 1:
                request_params["temperature"] = temperature

            # ä½¿ç”¨å®˜æ–¹ openai library ä¾†ç™¼é€è«‹æ±‚
            with OpenAI(api_key=OPENAI_API_KEY) as client:
                response = client.chat.completions.create(**request_params)
                output_text = response.choices[0].message.content.strip()
                # å­˜å…¥å¿«å–
                save_to_cache(cache_key, output_text)
                return output_text

        except Exception as e:
            print(f"  âŒ OpenAI API è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ERROR: OpenAI API è™•ç†å¤±æ•— - {e}"

    def call_google_api(
        self,
        model: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """
        å‘¼å« Google Gemini APIã€‚

        Args:
            model (str): è¦ä½¿ç”¨çš„ Google æ¨¡å‹åç¨± (e.g., "gemini-1.5-flash")ã€‚
            system_prompt (str): ç³»çµ±æç¤ºè©ã€‚
            user_content (str): ä½¿ç”¨è€…è¼¸å…¥çš„å…§å®¹ã€‚
            is_reviewer (bool): æ¨™è¨˜æ­¤å‘¼å«æ˜¯å¦ç‚ºè©•å¯©ç”¨é€”ã€‚

        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„æ–‡å­—çµæœæˆ–éŒ¯èª¤è¨Šæ¯ã€‚
        """
        # æª¢æŸ¥å¿«å–
        full_prompt = f"{system_prompt}\n\n{user_content}"
        cache_params = {
            "provider": "google",
            "model": model,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params, prompt=full_prompt)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… Google ({model}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        # æª¢æŸ¥ API é‡‘é‘°
        if not GOOGLE_API_KEY or GOOGLE_API_KEY == "your_google_api_key_here":
            return "ERROR: Google API é‡‘é‘°æœªè¨­å®š"

        # çµ„åˆ API URL
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GOOGLE_API_KEY}"
        headers = {"Content-Type": "application/json"}

        # å–å¾— temperature è¨­å®š
        temperature = REVIEWER_TEMPERATURE.get(model, 0.1)

        # Gemini API çš„è³‡æ–™æ ¼å¼èˆ‡ OpenAI ä¸åŒï¼Œéœ€è¦å°‡ç³»çµ±å’Œä½¿ç”¨è€…æç¤ºè©åˆä½µ
        data = {
            "contents": [{"parts": [{"text": f"{system_prompt}\n\n{user_content}"}]}]
        }

        # åŒæ¨£ï¼Œåªæœ‰åœ¨éœ€è¦æ™‚æ‰åŠ å…¥ temperature åƒæ•¸
        if temperature is not None and temperature != 1:
            data["generationConfig"] = {"temperature": temperature}

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« Google API ({model})...")
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()

            result = response.json()
            # è§£æ Gemini å›æ‡‰çš„ç‰¹å®šçµæ§‹
            output_text = result["candidates"][0]["content"]["parts"][0]["text"].strip()
            save_to_cache(cache_key, output_text)
            return output_text

        except Exception as e:
            print(f"  âŒ Google API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: Google API å¤±æ•— - {e}"

    def call_openrouter_api(
        self,
        model: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """
        å‘¼å« OpenRouter APIã€‚

        OpenRouter æ˜¯ä¸€å€‹æ¨¡å‹è·¯ç”±æœå‹™ï¼Œå¯ä»¥ç”¨ç›¸åŒçš„ API æ ¼å¼å‘¼å«ä¸åŒä¾›æ‡‰å•†çš„æ¨¡å‹ã€‚

        Args:
            model (str): è¦ä½¿ç”¨çš„ OpenRouter æ¨¡å‹è­˜åˆ¥ç¢¼ (e.g., "mistralai/mistral-7b-instruct")ã€‚
            system_prompt (str): ç³»çµ±æç¤ºè©ã€‚
            user_content (str): ä½¿ç”¨è€…è¼¸å…¥çš„å…§å®¹ã€‚
            is_reviewer (bool): æ¨™è¨˜æ­¤å‘¼å«æ˜¯å¦ç‚ºè©•å¯©ç”¨é€”ã€‚

        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„æ–‡å­—çµæœæˆ–éŒ¯èª¤è¨Šæ¯ã€‚
        """
        # æª¢æŸ¥å¿«å–
        full_prompt = f"{system_prompt}\n\n{user_content}"
        cache_params = {
            "provider": "openrouter",
            "model": model,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params, prompt=full_prompt)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… OpenRouter ({model}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        # æª¢æŸ¥ API é‡‘é‘°
        if (
            not OPENROUTER_API_KEY
            or OPENROUTER_API_KEY == "your_openrouter_api_key_here"
        ):
            return "ERROR: OpenRouter API é‡‘é‘°æœªè¨­å®š"

        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "HTTP-Referer": "http://localhost",  # å»ºè­°æ€§æ¨™é ­ï¼Œç”¨æ–¼åˆ†æ
            "X-Title": "Ollama Model Evaluator",  # å»ºè­°æ€§æ¨™é ­ï¼Œç”¨æ–¼åˆ†æ
        }

        # å–å¾— temperature è¨­å®š
        temperature = REVIEWER_TEMPERATURE.get(model, 0.1)

        # OpenRouter çš„ API æ ¼å¼èˆ‡ OpenAI ç›¸å®¹
        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content},
            ],
        }

        if temperature is not None and temperature != 1:
            data["temperature"] = temperature

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« OpenRouter API ({model})...")
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()

            result = response.json()
            output_text = result["choices"][0]["message"]["content"].strip()
            save_to_cache(cache_key, output_text)
            return output_text
        except Exception as e:
            print(f"  âŒ OpenRouter API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: OpenRouter API å¤±æ•— - {e}"

    def call_replicate_api(
        self,
        model_version: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """
        å‘¼å« Replicate APIã€‚

        Replicate çš„ API æ˜¯éåŒæ­¥çš„ã€‚éœ€è¦å…ˆç™¼é€ä¸€å€‹è«‹æ±‚ä¾†å•Ÿå‹•é æ¸¬ï¼Œ
        ç„¶å¾Œè¼ªè©¢å¦ä¸€å€‹ç«¯é»ä¾†ç²å–çµæœã€‚

        Args:
            model_version (str): Replicate æ¨¡å‹çš„ç‰ˆæœ¬è­˜åˆ¥ç¢¼ (æ ¼å¼é€šå¸¸æ˜¯ "owner/model_name:version_hash")ã€‚
            system_prompt (str): ç³»çµ±æç¤ºè©ã€‚
            user_content (str): ä½¿ç”¨è€…è¼¸å…¥çš„å…§å®¹ã€‚
            is_reviewer (bool): æ¨™è¨˜æ­¤å‘¼å«æ˜¯å¦ç‚ºè©•å¯©ç”¨é€”ã€‚

        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„æ–‡å­—çµæœæˆ–éŒ¯èª¤è¨Šæ¯ã€‚
        """
        # æª¢æŸ¥å¿«å–
        full_prompt = f"{system_prompt}\n\nUser: {user_content}\nAssistant:"
        cache_params = {
            "provider": "replicate",
            "model_version": model_version,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params, prompt=full_prompt)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… Replicate ({model_version}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        # æª¢æŸ¥ API é‡‘é‘°
        if not REPLICATE_API_KEY or REPLICATE_API_KEY == "your_replicate_api_key_here":
            return "ERROR: Replicate API é‡‘é‘°æœªè¨­å®š"

        url = "https://api.replicate.com/v1/predictions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Token {REPLICATE_API_KEY}",
        }

        # æ ¹æ“šå¸¸è¦‹çš„ Replicate æ¨¡å‹æ ¼å¼å»ºæ§‹è¼¸å…¥ã€‚
        # å°æ–¼é¡èŠå¤©æ¨¡å‹ï¼Œé€šå¸¸éœ€è¦å°‡ç³»çµ±å’Œä½¿ç”¨è€…æç¤ºè©çµ„åˆèµ·ä¾†ã€‚
        full_prompt = f"{system_prompt}\n\nUser: {user_content}\nAssistant:"

        # å–å¾— temperature è¨­å®š
        temperature = REVIEWER_TEMPERATURE.get(model_version, 0.1)

        # æº–å‚™è¼¸å…¥è³‡æ–™ï¼Œé€™éƒ¨åˆ†å¯èƒ½éœ€è¦æ ¹æ“šå…·é«”æ¨¡å‹é€²è¡Œèª¿æ•´
        input_data = {
            "prompt": full_prompt,
            "system_prompt": system_prompt,
            "max_new_tokens": 1024,  # ç¯„ä¾‹åƒæ•¸ï¼Œå¯èª¿æ•´
        }

        if temperature is not None and temperature != 1:
            input_data["temperature"] = temperature

        # æº–å‚™å®Œæ•´çš„è«‹æ±‚è³‡æ–™ï¼Œéœ€è¦æä¾›æ¨¡å‹çš„ç‰ˆæœ¬ hash
        data = {
            "version": model_version.split(":")[-1]
            if ":" in model_version
            else model_version,
            "input": input_data,
        }

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« Replicate API ({model_version})...")
            # æ­¥é©Ÿ 1: ç™¼é€è«‹æ±‚ä»¥å•Ÿå‹•é æ¸¬
            response = requests.post(url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            prediction_start_result = response.json()

            prediction_id = prediction_start_result.get("id")
            if not prediction_id:
                return f"ERROR: Replicate API æœªè¿”å›æœ‰æ•ˆçš„ prediction ID - {prediction_start_result.get('detail', 'ç„¡è©³ç´°éŒ¯èª¤')}"

            # æ­¥é©Ÿ 2: è¼ªè©¢ API ä»¥ç²å–çµæœ
            prediction_url: str = f"https://api.replicate.com/v1/predictions/{prediction_id}"
            max_retries = 20  # æœ€å¤šé‡è©¦ 20 æ¬¡ (ç´„ 100 ç§’)
            retry_interval = 5  # æ¯æ¬¡é‡è©¦é–“éš” 5 ç§’

            for _ in range(max_retries):
                time.sleep(retry_interval)
                poll_response = requests.get(
                    prediction_url, headers=headers, timeout=30
                )
                poll_response.raise_for_status()
                poll_result = poll_response.json()

                status = poll_result.get("status")
                if status == "succeeded":
                    # è™•ç†æˆåŠŸçš„å›æ‡‰ã€‚è¼¸å‡ºæ ¼å¼å¯èƒ½æ˜¯ä¸€å€‹å­—ä¸²åˆ—è¡¨æˆ–å–®ä¸€å­—ä¸²ã€‚
                    output = poll_result.get("output")
                    if isinstance(output, list):
                        output_text = "".join(output).strip()
                    elif isinstance(output, str):
                        output_text = output.strip()
                    else:
                        return (
                            f"ERROR: Replicate API è¿”å›äº†æœªçŸ¥çš„è¼¸å‡ºæ ¼å¼: {type(output)}"
                        )

                    save_to_cache(cache_key, output_text)
                    return output_text
                elif status == "failed" or status == "canceled":
                    error_detail = poll_result.get("error", "æœªçŸ¥éŒ¯èª¤")
                    return f"ERROR: Replicate é æ¸¬å¤±æ•—æˆ–å–æ¶ˆ - {error_detail}"
                # å¦‚æœç‹€æ…‹æ˜¯ "starting" æˆ– "processing"ï¼Œå‰‡ç¹¼çºŒè¼ªè©¢

            return f"ERROR: Replicate é æ¸¬è¶…æ™‚ ({model_version})"

        except requests.exceptions.RequestException as e:
            print(f"  âŒ Replicate API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: Replicate API è«‹æ±‚å¤±æ•— - {e}"
        except Exception as e:
            print(f"  âŒ Replicate API è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ERROR: Replicate API éŒ¯èª¤ - {e}"

    def evaluate_with_reviewer(
        self,
        reviewer_type: str,
        reviewer_model: str,
        task: str,
        original_text: str,
        model_output: str,
    ) -> Tuple[int, str]:
        """
        ä½¿ç”¨æŒ‡å®šçš„è©•å¯©æ¨¡å‹å° Ollama æ¨¡å‹çš„è¼¸å‡ºé€²è¡Œè©•åˆ†ã€‚

        å®ƒæœƒæ ¹æ“šä»»å‹™é¡å‹ï¼ˆç¿»è­¯æˆ–æ‘˜è¦ï¼‰é¸æ“‡å°æ‡‰çš„è©•åˆ†æ¨™æº–ï¼ˆç³»çµ±æç¤ºè©ï¼‰ï¼Œ
        ç„¶å¾Œå‘¼å«ç›¸æ‡‰çš„é›²ç«¯ API é€²è¡Œè©•å¯©ï¼Œæœ€å¾Œè§£æè©•å¯©æ¨¡å‹çš„å›æ‡‰ä»¥æå–åˆ†æ•¸å’Œè©•èªã€‚

        Args:
            reviewer_type (str): è©•å¯©æ¨¡å‹çš„ä¾›æ‡‰å•† (e.g., "openai", "gemini")ã€‚
            reviewer_model (str): è©•å¯©æ¨¡å‹çš„å…·é«”åç¨±ã€‚
            task (str): è¢«è©•åˆ†çš„ä»»å‹™åç¨±ã€‚
            original_text (str): åŸå§‹è¼¸å…¥æ–‡æœ¬ã€‚
            model_output (str): Ollama æ¨¡å‹çš„è¼¸å‡ºçµæœã€‚

        Returns:
            Tuple[int, str]: ä¸€å€‹åŒ…å« (åˆ†æ•¸, è©•èª) çš„å…ƒçµ„ã€‚
                             å¦‚æœè§£æå¤±æ•—ï¼Œåˆ†æ•¸æœƒè¨­ç‚º 1ï¼Œè©•èªæœƒæ˜¯éŒ¯èª¤è¨Šæ¯ã€‚
        """

        # æ ¹æ“šä»»å‹™é¡å‹é¸æ“‡ä¸åŒçš„ç³»çµ±æç¤ºè©å’Œè©•åˆ†æ¨™æº–
        if task == "translate":
            system_prompt = """ä½ æ˜¯å°ˆæ¥­çš„ç¿»è­¯è©•å¯©å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–å°ç¿»è­¯çµæœè©•åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š
è©•åˆ†æ¨™æº–ï¼š
- é€šé †æ€§ï¼ˆ1-3åˆ†ï¼‰ï¼šç¿»è­¯æ˜¯å¦è‡ªç„¶æµæš¢ï¼Œç¬¦åˆä¸­æ–‡è¡¨é”ç¿’æ…£
- æº–ç¢ºæ€§ï¼ˆ1-3åˆ†ï¼‰ï¼šæ˜¯å¦æœ‰ç¿»è­¯éŒ¯èª¤ã€éºæ¼æˆ–èª¤è§£
- éµå¾ªæŒ‡ä»¤(1-2åˆ†)ï¼šæ˜¯å¦å®Œå…¨éµå¾ªæŒ‡ä»¤ï¼Œä»¥ç¹é«”ä¸­æ–‡å›è¦†
- å°ˆæ¥­è¡“èªè™•ç†ï¼ˆ1-2åˆ†ï¼‰ï¼šè‹±æ–‡å°ˆæ¥­è¡“èªæ˜¯å¦é©ç•¶ä¿ç•™


è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š
åˆ†æ•¸: [1-10çš„æ•´æ•¸]
è©•èª: [ç°¡çŸ­è©•èªï¼Œèªªæ˜è©•åˆ†ç†ç”±]"""

            user_content = f"""åŸæ–‡ï¼š
{original_text}

ç¿»è­¯çµæœï¼š
{model_output}

è«‹è©•åˆ†ä¸¦çµ¦å‡ºè©•èªã€‚"""

        elif task == "summarize":
            system_prompt = """ä½ æ˜¯å°ˆæ¥­çš„æ‘˜è¦è©•å¯©å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–å°æ‘˜è¦çµæœè©•åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š

è©•åˆ†æ¨™æº–ï¼š
- é‡é»æ¶µè“‹ï¼ˆ1-3åˆ†ï¼‰ï¼šé‡è¦è­°é¡Œå’Œé—œéµæˆæœæ˜¯å¦æœ‰æåŠ
- è¡¨é”æ¸…æ¥šï¼ˆ1-3åˆ†ï¼‰ï¼šæ‘˜è¦æ˜¯å¦æ¢ç†åˆ†æ˜ã€æ˜“æ–¼ç†è§£  
- éµå¾ªæŒ‡ä»¤(1-2åˆ†)ï¼šæ˜¯å¦å®Œå…¨éµå¾ªæŒ‡ä»¤ï¼Œä»¥ç¹é«”ä¸­æ–‡å›è¦†
- ç°¡æ½”æ€§ï¼ˆ1-2åˆ†ï¼‰ï¼šæ˜¯å¦é¿å…å†—é¤˜ï¼Œåˆ‡ä¸­è¦é»

è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š
åˆ†æ•¸: [1-10çš„æ•´æ•¸]
è©•èª: [ç°¡çŸ­è©•èªï¼Œèªªæ˜è©•åˆ†ç†ç”±]"""

            user_content = f"""åŸæ–‡ï¼š
{original_text}

æ‘˜è¦çµæœï¼š
{model_output}

è«‹è©•åˆ†ä¸¦çµ¦å‡ºè©•èªã€‚"""

        # å‘¼å«å°æ‡‰çš„è©•å¯© API
        if reviewer_type == "openai":
            response = self.call_openai_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        elif reviewer_type == "gemini":
            response = self.call_google_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        elif reviewer_type == "openrouter":
            response = self.call_openrouter_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        elif reviewer_type == "replicate":
            response = self.call_replicate_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        else:
            return 0, "ERROR: ä¸æ”¯æ´çš„è©•å¯©æ¨¡å‹é¡å‹"

        # è§£æè©•å¯©æ¨¡å‹è¿”å›çš„æ–‡å­—ï¼Œæå–åˆ†æ•¸å’Œè©•èª
        try:
            lines = response.split("\n")
            score = 0
            comment = "ç„¡è©•èª"
            comment_started = False
            comment_lines = []
            found_score = False
            found_comment = False

            for line in lines:
                line = line.strip()
                # å°‹æ‰¾ "åˆ†æ•¸:" æˆ–ç°¡é«”çš„ "åˆ†æ•°:" é–‹é ­çš„è¡Œ
                if line.startswith("åˆ†æ•¸:") or line.startswith("åˆ†æ•°:"):
                    score_text = line.split(":")[1].strip()
                    # å¾æ–‡å­—ä¸­æå–æ‰€æœ‰æ•¸å­—éƒ¨åˆ†ä¸¦è½‰æ›ç‚ºæ•´æ•¸
                    score = int("".join(filter(str.isdigit, score_text)))
                    found_score = True
                # å°‹æ‰¾ "è©•èª:" æˆ–ç°¡é«”çš„ "è¯„è¯­:" é–‹é ­çš„è¡Œ
                elif line.startswith("è©•èª:") or line.startswith("è¯„è¯­:"):
                    # è™•ç†è©•èªå¯èƒ½è·¨è¶Šå¤šè¡Œçš„æƒ…æ³
                    initial_comment = line.split(":", 1)[1].strip()
                    if initial_comment:
                        comment_lines.append(initial_comment)
                    comment_started = True
                    found_comment = True
                elif comment_started and line:
                    comment_lines.append(line)

            if comment_lines:
                comment = " ".join(comment_lines) # å°‡å¤šè¡Œè©•èªåˆä½µç‚ºä¸€è¡Œ

            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šæ ¼å¼çš„åˆ†æ•¸å’Œè©•èªï¼Œå‰‡èªç‚ºè§£æå¤±æ•—
            if not found_score and not found_comment:
                return 1, f"è§£æå¤±æ•—: {response[:100]}..."

            # ç¢ºä¿åˆ†æ•¸åœ¨ 1 åˆ° 10 çš„æœ‰æ•ˆç¯„åœå…§
            score = max(1, min(10, score))
            return score, comment

        except Exception as e:
            print(f"  âš ï¸  è§£æè©•åˆ†å¤±æ•—: {e}")
            return 1, f"è§£æå¤±æ•—: {response[:100]}..." # è¿”å›éƒ¨åˆ†å›æ‡‰å…§å®¹ä»¥ä¾›é™¤éŒ¯

    def run_evaluation(self):
        """
        åŸ·è¡Œå®Œæ•´çš„è©•æ¯”æµç¨‹ã€‚

        é€™æ˜¯æ•´å€‹è©•æ¯”å·¥ä½œçš„æ ¸å¿ƒå”èª¿å‡½å¼ã€‚
        """
        print("ğŸš€ é–‹å§‹æ¨¡å‹è©•æ¯”...")

        # æ­¥é©Ÿ 1: è®€å–è¼¸å…¥æ–‡æœ¬
        input_text = self.read_input_text()
        print(f"ğŸ“– å·²è®€å–æ¸¬è©¦æ–‡æœ¬ ({len(input_text)} å­—å…ƒ)")

        # æ­¥é©Ÿ 2: éæ­·æ‰€æœ‰è¦æ¸¬è©¦çš„ Ollama æ¨¡å‹ï¼ŒåŸ·è¡Œå„é …ä»»å‹™
        for model in OLLAMA_MODELS_TO_COMPARE:
            print(f"\nğŸ” æ­£åœ¨æ¸¬è©¦æ¨¡å‹: {model}")
            self.results[model] = {}

            for task in SUPPORTED_TASKS.keys():
                print(f"  ğŸ“ åŸ·è¡Œä»»å‹™: {task}")

                # å‘¼å« Ollama API ä¸¦å„²å­˜çµæœ
                result = self.call_ollama_api(model, task, input_text)
                self.results[model][task] = result

                if result.startswith("ERROR:"):
                    print(f"  âŒ ä»»å‹™å¤±æ•—: {result}")
                else:
                    print(f"  âœ… ä»»å‹™å®Œæˆ ({len(result)} å­—å…ƒ)")

                time.sleep(1)  # çŸ­æš«å»¶é²ï¼Œé¿å…å° API é€ æˆéå¤§å£“åŠ›

        # æ­¥é©Ÿ 3: éæ­·æ‰€æœ‰è©•å¯©æ¨¡å‹ï¼Œå°å‰ä¸€æ­¥çš„çµæœé€²è¡Œè©•åˆ†
        print("\nâš–ï¸  é–‹å§‹è©•å¯©éšæ®µ...")
        self.evaluation_scores = {}

        for reviewer_config in REVIEWER_MODELS:
            reviewer_provider = reviewer_config["provider"]
            reviewer_model = reviewer_config["model"]
            
            # å»ºç«‹ä¸€å€‹å°æª”æ¡ˆç³»çµ±å‹å–„çš„å”¯ä¸€è©•å¯©è€… ID
            reviewer_id = f"{reviewer_provider}_{reviewer_model.replace('/', '_').replace(':', '_').replace('-', '_')}"

            print(f"\nğŸ¯ ä½¿ç”¨è©•å¯©æ¨¡å‹: {reviewer_provider} ({reviewer_model})")
            self.evaluation_scores[reviewer_id] = {}

            for model in OLLAMA_MODELS_TO_COMPARE:
                self.evaluation_scores[reviewer_id][model] = {}

                for task in SUPPORTED_TASKS.keys():
                    # å¦‚æœæ¨¡å‹åŸ·è¡Œå¤±æ•—ï¼Œå‰‡ç›´æ¥çµ¦ 0 åˆ†
                    if self.results[model][task].startswith("ERROR:"):
                        score, comment = 0, "æ¨¡å‹åŸ·è¡Œå¤±æ•—"
                    else:
                        print(f"  ğŸ“Š è©•å¯© {model} çš„ {task} çµæœ...")
                        # å‘¼å«è©•å¯©å‡½å¼
                        score, comment = self.evaluate_with_reviewer(
                            reviewer_provider,
                            reviewer_model,
                            task,
                            input_text,
                            self.results[model][task],
                        )

                    # å„²å­˜è©•åˆ†çµæœ
                    self.evaluation_scores[reviewer_id][model][task] = {
                        "score": score,
                        "comment": comment,
                    }

                    print(f"    åˆ†æ•¸: {score}/10")
                    time.sleep(1)  # é¿å… API é™åˆ¶

    def generate_report(self):
        """
        ç”Ÿæˆæœ€çµ‚çš„è©•æ¯”å ±è¡¨ï¼ˆMarkdown å’Œ HTMLï¼‰ã€‚
        """
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå ±è¡¨...")
        
        # ä½¿ç”¨ç•¶å‰æ™‚é–“å»ºç«‹ç¨ä¸€ç„¡äºŒçš„æ™‚é–“æˆ³è¨˜ï¼Œç”¨æ–¼æª”å
        timestamp = datetime.now().strftime("%Y%m%d%H%M")
        
        # æ­¥é©Ÿ 1: å„ªå…ˆç”Ÿæˆåœ–è¡¨æª”æ¡ˆï¼Œå› ç‚º Markdown å ±è¡¨éœ€è¦å¼•ç”¨å®ƒå€‘
        self.create_charts(timestamp)
        
        # æ­¥é©Ÿ 2: å»ºç«‹ Markdown å ±è¡¨çš„å®Œæ•´å…§å®¹
        report_content = self.create_markdown_report(timestamp)
        
        # æ­¥é©Ÿ 3: å°‡ Markdown å…§å®¹å¯«å…¥æª”æ¡ˆ
        report_md_path = f"reports/evaluation_report_{timestamp}.md"
        with open(report_md_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        print(f"âœ… Markdown å ±è¡¨å·²ç”Ÿæˆ: {report_md_path}")
        
        # æ­¥é©Ÿ 4: å°‡ Markdown æª”æ¡ˆè½‰æ›ç‚º HTML
        report_html_path = f"reports/evaluation_report_{timestamp}.html"
        convert_markdown_to_html(report_md_path, report_html_path)
        print(f"âœ… HTML å ±è¡¨å·²ç”Ÿæˆ: {report_html_path}")
        
        return report_md_path, report_html_path

    def create_markdown_report(self, timestamp: str) -> str:
        """
        çµ„åˆå‡º Markdown æ ¼å¼çš„å ±è¡¨å­—ä¸²ã€‚

        Args:
            timestamp (str): ç”¨æ–¼é€£çµåœ–æª”çš„æ™‚é–“æˆ³è¨˜ã€‚

        Returns:
            str: å®Œæ•´çš„ Markdown å ±è¡¨å…§å®¹ã€‚
        """
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # å ±è¡¨æ¨™é ­
        content = f"""# Ollama æ¨¡å‹è©•æ¯”å ±è¡¨

**ç”Ÿæˆæ™‚é–“**: {current_time}

## è©•æ¯”æ¦‚è¦

æœ¬æ¬¡è©•æ¯”æ¸¬è©¦äº† {len(OLLAMA_MODELS_TO_COMPARE)} å€‹ Ollama æ¨¡å‹åœ¨å…©å€‹ä»»å‹™ä¸Šçš„è¡¨ç¾ï¼š
- **ç¿»è­¯ä»»å‹™** (Translate): è‹±æ–‡ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡
- **æ‘˜è¦ä»»å‹™** (Summarize): æœƒè­°è¨˜éŒ„æ‘˜è¦

### æ¸¬è©¦æ¨¡å‹æ¸…å–®
"""
        # åˆ—å‡ºæ‰€æœ‰è¢«æ¸¬è©¦çš„ Ollama æ¨¡å‹
        for i, model in enumerate(OLLAMA_MODELS_TO_COMPARE, 1):
            content += f"{i}. `{model}`\n"

        # åˆ—å‡ºæ‰€æœ‰è©•å¯©æ¨¡å‹
        content += "\n### è©•å¯©æ¨¡å‹\n"
        for reviewer_config in REVIEWER_MODELS:
            reviewer_provider = reviewer_config["provider"]
            reviewer_model = reviewer_config["model"]
            content += f"- **{reviewer_provider.upper()}**: `{reviewer_model}`\n"

        # ç‚ºæ¯å€‹è©•å¯©æ¨¡å‹å»ºç«‹ä¸€å€‹è©•åˆ†è¡¨æ ¼
        for reviewer_config in REVIEWER_MODELS:
            reviewer_provider = reviewer_config["provider"]
            reviewer_model = reviewer_config["model"]
            reviewer_id = f"{reviewer_provider}_{reviewer_model.replace('/', '_').replace(':', '_').replace('-', '_')}"
            
            if reviewer_id not in self.evaluation_scores:
                continue

            content += f"\n## {reviewer_provider.upper()} ({reviewer_model}) è©•å¯©çµæœ\n\n"
            content += "| æ¨¡å‹ | ç¿»è­¯åˆ†æ•¸ | ç¿»è­¯è©•èª | æ‘˜è¦åˆ†æ•¸ | æ‘˜è¦è©•èª | å¹³å‡åˆ†æ•¸ |\n"
            content += "|------|----------|----------|----------|----------|----------|\n"

            # å¡«å…¥æ¯å€‹æ¨¡å‹çš„åˆ†æ•¸å’Œè©•èª
            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_id]:
                    translate_score = self.evaluation_scores[reviewer_id][model].get("translate", {}).get("score", 0)
                    translate_comment = self.evaluation_scores[reviewer_id][model].get("translate", {}).get("comment", "N/A")
                    summarize_score = self.evaluation_scores[reviewer_id][model].get("summarize", {}).get("score", 0)
                    summarize_comment = self.evaluation_scores[reviewer_id][model].get("summarize", {}).get("comment", "N/A")
                    avg_score = (translate_score + summarize_score) / 2
                    content += f"| `{model}` | {translate_score} | {translate_comment} | {summarize_score} | {summarize_comment} | {avg_score:.1f} |\n"

        # çµ±è¨ˆåˆ†æå€å¡Š
        content += "\n## çµ±è¨ˆåˆ†æ\n\n"
        for reviewer_config in REVIEWER_MODELS:
            reviewer_provider = reviewer_config["provider"]
            reviewer_model = reviewer_config["model"]
            reviewer_id = f"{reviewer_provider}_{reviewer_model.replace('/', '_').replace(':', '_').replace('-', '_')}"
            
            if reviewer_id not in self.evaluation_scores:
                continue

            content += f"### {reviewer_provider.upper()} ({reviewer_model}) è©•å¯©çµ±è¨ˆ\n\n"

            # è¨ˆç®—æ¯å€‹ä»»å‹™çš„å¹³å‡åˆ†ã€æœ€é«˜åˆ†ã€æœ€ä½åˆ†
            translate_scores = [self.evaluation_scores[reviewer_id][m].get("translate", {}).get("score", 0) for m in OLLAMA_MODELS_TO_COMPARE if m in self.evaluation_scores[reviewer_id] and self.evaluation_scores[reviewer_id][m].get("translate", {}).get("score", 0) > 0]
            summarize_scores = [self.evaluation_scores[reviewer_id][m].get("summarize", {}).get("score", 0) for m in OLLAMA_MODELS_TO_COMPARE if m in self.evaluation_scores[reviewer_id] and self.evaluation_scores[reviewer_id][m].get("summarize", {}).get("score", 0) > 0]

            if translate_scores:
                content += f"- **ç¿»è­¯ä»»å‹™å¹³å‡åˆ†æ•¸**: {np.mean(translate_scores):.2f}\n"
                content += f"- **ç¿»è­¯ä»»å‹™æœ€é«˜åˆ†**: {max(translate_scores)}\n"
                content += f"- **ç¿»è­¯ä»»å‹™æœ€ä½åˆ†**: {min(translate_scores)}\n"

            if summarize_scores:
                content += f"- **æ‘˜è¦ä»»å‹™å¹³å‡åˆ†æ•¸**: {np.mean(summarize_scores):.2f}\n"
                content += f"- **æ‘˜è¦ä»»å‹™æœ€é«˜åˆ†**: {max(summarize_scores)}\n"
                content += f"- **æ‘˜è¦ä»»å‹™æœ€ä½åˆ†**: {min(summarize_scores)}\n"
            content += "\n"

        # è¦–è¦ºåŒ–åœ–è¡¨å€å¡Š
        content += "## è¦–è¦ºåŒ–åœ–è¡¨\n\n"
        for reviewer_config in REVIEWER_MODELS:
            reviewer_provider = reviewer_config["provider"]
            reviewer_model = reviewer_config["model"]
            reviewer_id = f"{reviewer_provider}_{reviewer_model.replace('/', '_').replace(':', '_').replace('-', '_')}"
            
            if reviewer_id not in self.evaluation_scores:
                continue
            
            chart_path = f"chart_{reviewer_id}_{timestamp}.png"
            content += f"### {reviewer_provider.upper()} ({reviewer_model}) è©•å¯©çµæœåœ–è¡¨\n\n"
            content += f"![{reviewer_provider.upper()} ({reviewer_model}) è©•å¯©çµæœ]({chart_path})\n\n"

        # é™„éŒ„ï¼šæ¨¡å‹åŸå§‹è¼¸å‡ºçµæœ
        content += "## æ¨¡å‹è¼¸å‡ºçµæœ\n\n"
        for model in OLLAMA_MODELS_TO_COMPARE:
            content += f"### {model}\n\n"
            for task in SUPPORTED_TASKS.keys():
                content += f"#### {task.capitalize()} çµæœ\n\n"
                content += "```\n"
                content += self.results[model][task]
                content += "\n```\n\n"

        return content

    def create_charts(self, timestamp: str):
        """
        ä½¿ç”¨ Matplotlib ç”Ÿæˆæ¯”è¼ƒå„æ¨¡å‹è¡¨ç¾çš„é•·æ¢åœ–ï¼Œä¸¦å„²å­˜ç‚º PNG æª”æ¡ˆã€‚

        Args:
            timestamp (str): ç”¨æ–¼ç”Ÿæˆå”¯ä¸€æª”åçš„æ™‚é–“æˆ³è¨˜ã€‚
        """
        print("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆåœ–è¡¨...")

        for reviewer_config in REVIEWER_MODELS:
            reviewer_provider = reviewer_config["provider"]
            reviewer_model = reviewer_config["model"]
            reviewer_id = f"{reviewer_provider}_{reviewer_model.replace('/', '_').replace(':', '_').replace('-', '_')}"
            
            if reviewer_id not in self.evaluation_scores:
                continue

            # æº–å‚™ç¹ªåœ–æ‰€éœ€çš„æ•¸æ“š
            models = []
            translate_scores = []
            summarize_scores = []

            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_id]:
                    # ç°¡åŒ–æ¨¡å‹åç¨±ä»¥åˆ©é¡¯ç¤º
                    models.append(model.replace("hf.co/mradermacher/", "").replace(":Q4_K_M", ""))
                    translate_scores.append(self.evaluation_scores[reviewer_id][model].get("translate", {}).get("score", 0))
                    summarize_scores.append(self.evaluation_scores[reviewer_id][model].get("summarize", {}).get("score", 0))

            if not models:
                continue

            # é–‹å§‹ç¹ªåœ–
            x = np.arange(len(models))  # X è»¸åº§æ¨™
            width = 0.35  # é•·æ¢å¯¬åº¦

            fig, ax = plt.subplots(figsize=(12, 8)) # è¨­å®šç•«å¸ƒå¤§å°
            # ç¹ªè£½ç¿»è­¯åˆ†æ•¸çš„é•·æ¢
            bars1 = ax.bar(x - width / 2, translate_scores, width, label="ç¿»è­¯ (Translate)", alpha=0.8)
            # ç¹ªè£½æ‘˜è¦åˆ†æ•¸çš„é•·æ¢
            bars2 = ax.bar(x + width / 2, summarize_scores, width, label="æ‘˜è¦ (Summarize)", alpha=0.8)

            # è¨­å®šåœ–è¡¨æ¨™é¡Œã€åº§æ¨™è»¸æ¨™ç±¤ç­‰
            ax.set_xlabel("æ¨¡å‹")
            ax.set_ylabel("åˆ†æ•¸")
            ax.set_title(f"æ¨¡å‹è©•æ¯”çµæœ - {reviewer_provider.upper()} ({reviewer_model}) è©•å¯©")
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45, ha="right") # X è»¸æ¨™ç±¤æ—‹è½‰ä»¥å…é‡ç–Š
            ax.legend()
            ax.set_ylim(0, 10) # Y è»¸ç¯„åœè¨­ç‚º 0-10

            # åœ¨æ¯å€‹é•·æ¢ä¸Šæ–¹é¡¯ç¤ºåˆ†æ•¸å€¼
            def autolabel(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax.annotate(
                        f"{height}",
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),  # å‚ç›´åç§» 3 é»
                        textcoords="offset points",
                        ha="center",
                        va="bottom",
                    )

            autolabel(bars1)
            autolabel(bars2)

            plt.tight_layout()  # è‡ªå‹•èª¿æ•´ç‰ˆé¢
            chart_path = f"reports/chart_{reviewer_id}_{timestamp}.png"
            plt.savefig(chart_path, dpi=300, bbox_inches="tight") # å„²å­˜åœ–æª”
            plt.close() # é—œé–‰ç•«å¸ƒï¼Œé‡‹æ”¾è³‡æº

            print(f"âœ… åœ–è¡¨å·²ç”Ÿæˆ: {chart_path}")


def main():
    """
    ä¸»ç¨‹å¼å…¥å£å‡½å¼ã€‚
    """
    print("ğŸ¯ Ollama æ¨¡å‹è©•æ¯”ç³»çµ±")
    print("=" * 50)

    # åŸ·è¡Œå‰çš„åŸºæœ¬æª¢æŸ¥
    if not OLLAMA_MODELS_TO_COMPARE:
        print("âŒ æœªè¨­å®šè¦è©•æ¯”çš„æ¨¡å‹ï¼Œè«‹æª¢æŸ¥ config.py")
        sys.exit(1)

    if not REVIEWER_MODELS:
        print("âŒ æœªè¨­å®šè©•å¯©æ¨¡å‹ï¼Œè«‹æª¢æŸ¥ config.py")
        sys.exit(1)

    # å»ºç«‹è©•æ¯”å™¨ç‰©ä»¶
    evaluator = ModelEvaluator()

    try:
        # åŸ·è¡Œè©•æ¯”æµç¨‹
        evaluator.run_evaluation()

        # ç”Ÿæˆå ±è¡¨
        md_path, html_path = evaluator.generate_report()

        # é¡¯ç¤ºæœ€çµ‚çµæœ
        print("\n" + "=" * 50)
        print("ğŸ‰ è©•æ¯”å®Œæˆï¼")
        print(f"ğŸ“„ Markdown å ±è¡¨: {md_path}")
        print(f"ğŸŒ HTML å ±è¡¨: {html_path}")
        print("=" * 50)

    except KeyboardInterrupt:
        # è™•ç†ä½¿ç”¨è€…æ‰‹å‹•ä¸­æ–· (Ctrl+C)
        print("\nâŒ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        sys.exit(1)
    except Exception as e:
        # æ•æ‰æ‰€æœ‰å…¶ä»–æœªé æœŸçš„éŒ¯èª¤
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)


# ç•¶æ­¤è…³æœ¬è¢«ç›´æ¥åŸ·è¡Œæ™‚ï¼Œæ‰å‘¼å« main() å‡½å¼
if __name__ == "__main__":
    main()
