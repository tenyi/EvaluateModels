#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import re
import sys
import time
import requests
from typing import Dict, List, Tuple, Any
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
from config import (
    OLLAMA_API_BASE_URL, 
    OLLAMA_MODELS_TO_COMPARE, 
    OPENAI_API_KEY, 
    GOOGLE_API_KEY,
    REVIEWER_MODELS, 
    SUPPORTED_TASKS
)
from markdown2html import convert_markdown_to_html

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ModelEvaluator:
    def __init__(self):
        self.results = {}
        self.evaluation_scores = {}
        
    def read_input_text(self, file_path: str = "input.txt") -> str:
        """è®€å–æ¸¬è©¦æ¨£æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {file_path}")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            sys.exit(1)

    def call_ollama_api(self, model: str, task: str, text: str) -> str:
        """å‘¼å« Ollama API"""
        url = f"{OLLAMA_API_BASE_URL}/api/chat"
        headers = {"Content-Type": "application/json"}
        
        prompt = SUPPORTED_TASKS[task]
        
        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": prompt},
                {"role": "user", "content": text}
            ],
            "stream": False
        }
        
        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« {model} åŸ·è¡Œ {task} ä»»å‹™...")
            response = requests.post(url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            
            result = response.json()
            text = result["message"]["content"].strip()
            text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
            return text
            
        except requests.exceptions.Timeout:
            print(f"  âš ï¸  {model} å›æ‡‰è¶…æ™‚")
            return "ERROR: å›æ‡‰è¶…æ™‚"
        except requests.exceptions.RequestException as e:
            print(f"  âŒ {model} API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: API å‘¼å«å¤±æ•— - {e}"
        except Exception as e:
            print(f"  âŒ {model} è™•ç†å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ERROR: è™•ç†å›æ‡‰å¤±æ•— - {e}"

    def call_openai_api(self, model: str, system_prompt: str, user_content: str) -> str:
        """å‘¼å« OpenAI API"""
        if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
            return "ERROR: OpenAI API é‡‘é‘°æœªè¨­å®š"
            
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENAI_API_KEY}"
        }
        
        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            "temperature": 0.1
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            text = result["choices"][0]["message"]["content"].strip()
            return text        
        except Exception as e:
            print(f"  âŒ OpenAI API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: OpenAI API å¤±æ•— - {e}"

    def call_google_api(self, model: str, system_prompt: str, user_content: str) -> str:
        """å‘¼å« Google Gemini API"""
        if not GOOGLE_API_KEY or GOOGLE_API_KEY == "your_google_api_key_here":
            return "ERROR: Google API é‡‘é‘°æœªè¨­å®š"
            
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GOOGLE_API_KEY}"
        headers = {"Content-Type": "application/json"}
        
        data = {
            "contents": [{
                "parts": [{
                    "text": f"{system_prompt}\n\n{user_content}"
                }]
            }],
            "generationConfig": {
                "temperature": 0.1
            }
        }
        
        try:
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            return result["candidates"][0]["content"]["parts"][0]["text"].strip()
            
        except Exception as e:
            print(f"  âŒ Google API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: Google API å¤±æ•— - {e}"

    def evaluate_with_reviewer(self, reviewer_type: str, reviewer_model: str, task: str, 
                             original_text: str, model_output: str) -> Tuple[int, str]:
        """ä½¿ç”¨è©•å¯©æ¨¡å‹è©•åˆ†"""
        
        if task == "translate":
            system_prompt = """ä½ æ˜¯å°ˆæ¥­çš„ç¿»è­¯è©•å¯©å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–å°ç¿»è­¯çµæœè©•åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š

è©•åˆ†æ¨™æº–ï¼š
- é€šé †æ€§ï¼ˆ1-3åˆ†ï¼‰ï¼šç¿»è­¯æ˜¯å¦è‡ªç„¶æµæš¢ï¼Œç¬¦åˆä¸­æ–‡è¡¨é”ç¿’æ…£
- æº–ç¢ºæ€§ï¼ˆ1-3åˆ†ï¼‰ï¼šæ˜¯å¦æœ‰ç¿»è­¯éŒ¯èª¤ã€éºæ¼æˆ–èª¤è§£
- éµå¾ªæŒ‡ä»¤(1-2åˆ†)ï¼šæ˜¯å¦å®Œå…¨éµå¾ªæŒ‡ä»¤ï¼Œä»¥ç¹é«”ä¸­æ–‡å›è¦†
- å°ˆæ¥­è¡“èªè™•ç†ï¼ˆ1-2åˆ†ï¼‰ï¼šè‹±æ–‡å°ˆæ¥­è¡“èªæ˜¯å¦é©ç•¶ä¿ç•™

è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š
åˆ†æ•¸: [1-10çš„æ•´æ•¸]
è©•èª: [ç°¡çŸ­è©•èªï¼Œèªªæ˜è©•åˆ†ç†ç”±]"""
            
            user_content = f"""åŸæ–‡ï¼š
{original_text}

ç¿»è­¯çµæœï¼š
{model_output}

è«‹è©•åˆ†ä¸¦çµ¦å‡ºè©•èªã€‚"""

        elif task == "summarize":
            system_prompt = """ä½ æ˜¯å°ˆæ¥­çš„æ‘˜è¦è©•å¯©å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–å°æ‘˜è¦çµæœè©•åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š

è©•åˆ†æ¨™æº–ï¼š
- é‡é»æ¶µè“‹ï¼ˆ1-3åˆ†ï¼‰ï¼šé‡è¦è­°é¡Œå’Œé—œéµæˆæœæ˜¯å¦æœ‰æåŠ
- è¡¨é”æ¸…æ¥šï¼ˆ1-3åˆ†ï¼‰ï¼šæ‘˜è¦æ˜¯å¦æ¢ç†åˆ†æ˜ã€æ˜“æ–¼ç†è§£  
- éµå¾ªæŒ‡ä»¤(1-2åˆ†)ï¼šæ˜¯å¦å®Œå…¨éµå¾ªæŒ‡ä»¤ï¼Œä»¥ç¹é«”ä¸­æ–‡å›è¦†
- ç°¡æ½”æ€§ï¼ˆ1-2åˆ†ï¼‰ï¼šæ˜¯å¦é¿å…å†—é¤˜ï¼Œåˆ‡ä¸­è¦é»

è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š
åˆ†æ•¸: [1-10çš„æ•´æ•¸]
è©•èª: [ç°¡çŸ­è©•èªï¼Œèªªæ˜è©•åˆ†ç†ç”±]"""
            
            user_content = f"""åŸæ–‡ï¼š
{original_text}

æ‘˜è¦çµæœï¼š
{model_output}

è«‹è©•åˆ†ä¸¦çµ¦å‡ºè©•èªã€‚"""
        
        # å‘¼å«å°æ‡‰çš„è©•å¯© API
        if reviewer_type == "openai":
            response = self.call_openai_api(reviewer_model, system_prompt, user_content)
        elif reviewer_type == "gemini":
            response = self.call_google_api(reviewer_model, system_prompt, user_content)
        else:
            return 0, "ERROR: ä¸æ”¯æ´çš„è©•å¯©æ¨¡å‹é¡å‹"
        
        # è§£æè©•åˆ†çµæœ
        try:
            lines = response.split('\n')
            score = 0
            comment = "ç„¡è©•èª"
            
            for line in lines:
                if line.startswith('åˆ†æ•¸:') or line.startswith('åˆ†æ•°:'):
                    score_text = line.split(':')[1].strip()
                    score = int(''.join(filter(str.isdigit, score_text)))
                elif line.startswith('è©•èª:') or line.startswith('è¯„è¯­:'):
                    comment = line.split(':', 1)[1].strip()
            
            # ç¢ºä¿åˆ†æ•¸åœ¨æœ‰æ•ˆç¯„åœå…§
            score = max(1, min(10, score))
            return score, comment
            
        except Exception as e:
            print(f"  âš ï¸  è§£æè©•åˆ†å¤±æ•—: {e}")
            return 5, f"è§£æå¤±æ•—: {response[:100]}..."

    def run_evaluation(self):
        """åŸ·è¡Œå®Œæ•´çš„è©•æ¯”æµç¨‹"""
        print("ğŸš€ é–‹å§‹æ¨¡å‹è©•æ¯”...")
        
        # è®€å–æ¸¬è©¦æ–‡æœ¬
        input_text = self.read_input_text()
        print(f"ğŸ“– å·²è®€å–æ¸¬è©¦æ–‡æœ¬ ({len(input_text)} å­—å…ƒ)")
        
        # å°æ¯å€‹æ¨¡å‹åŸ·è¡Œä»»å‹™
        for model in OLLAMA_MODELS_TO_COMPARE:
            print(f"\nğŸ” æ­£åœ¨æ¸¬è©¦æ¨¡å‹: {model}")
            self.results[model] = {}
            
            for task in SUPPORTED_TASKS.keys():
                print(f"  ğŸ“ åŸ·è¡Œä»»å‹™: {task}")
                
                # å‘¼å« Ollama æ¨¡å‹
                result = self.call_ollama_api(model, task, input_text)
                self.results[model][task] = result
                
                if result.startswith("ERROR:"):
                    print(f"  âŒ ä»»å‹™å¤±æ•—: {result}")
                else:
                    print(f"  âœ… ä»»å‹™å®Œæˆ ({len(result)} å­—å…ƒ)")
                
                # çŸ­æš«å»¶é²é¿å… API é™åˆ¶
                time.sleep(1)
        
        # ä½¿ç”¨è©•å¯©æ¨¡å‹è©•åˆ†
        print(f"\nâš–ï¸  é–‹å§‹è©•å¯©éšæ®µ...")
        self.evaluation_scores = {}
        
        for reviewer_type, reviewer_model in REVIEWER_MODELS.items():
            if reviewer_model is None:
                continue
                
            print(f"\nğŸ¯ ä½¿ç”¨è©•å¯©æ¨¡å‹: {reviewer_type} ({reviewer_model})")
            self.evaluation_scores[reviewer_type] = {}
            
            for model in OLLAMA_MODELS_TO_COMPARE:
                self.evaluation_scores[reviewer_type][model] = {}
                
                for task in SUPPORTED_TASKS.keys():
                    if self.results[model][task].startswith("ERROR:"):
                        score, comment = 0, "æ¨¡å‹åŸ·è¡Œå¤±æ•—"
                    else:
                        print(f"  ğŸ“Š è©•å¯© {model} çš„ {task} çµæœ...")
                        score, comment = self.evaluate_with_reviewer(
                            reviewer_type, reviewer_model, task, 
                            input_text, self.results[model][task]
                        )
                    
                    self.evaluation_scores[reviewer_type][model][task] = {
                        "score": score,
                        "comment": comment
                    }
                    
                    print(f"    åˆ†æ•¸: {score}/10")
                    time.sleep(1)  # é¿å… API é™åˆ¶

    def generate_report(self):
        """ç”Ÿæˆè©•æ¯”å ±è¡¨"""
        print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆå ±è¡¨...")
        
        # å…ˆç”Ÿæˆåœ–è¡¨
        self.create_charts()
        
        # å»ºç«‹å ±è¡¨å…§å®¹
        report_content = self.create_markdown_report()
        
        # å¯«å…¥ Markdown æª”æ¡ˆ
        report_md_path = "reports/evaluation_report.md"
        with open(report_md_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"âœ… Markdown å ±è¡¨å·²ç”Ÿæˆ: {report_md_path}")
        
        # è½‰æ›ç‚º HTML
        report_html_path = "reports/evaluation_report.html"
        convert_markdown_to_html(report_md_path, report_html_path)
        print(f"âœ… HTML å ±è¡¨å·²ç”Ÿæˆ: {report_html_path}")
        
        return report_md_path, report_html_path

    def create_markdown_report(self) -> str:
        """å»ºç«‹ Markdown æ ¼å¼çš„å ±è¡¨"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        content = f"""# Ollama æ¨¡å‹è©•æ¯”å ±è¡¨

**ç”Ÿæˆæ™‚é–“**: {current_time}

## è©•æ¯”æ¦‚è¦

æœ¬æ¬¡è©•æ¯”æ¸¬è©¦äº† {len(OLLAMA_MODELS_TO_COMPARE)} å€‹ Ollama æ¨¡å‹åœ¨å…©å€‹ä»»å‹™ä¸Šçš„è¡¨ç¾ï¼š
- **ç¿»è­¯ä»»å‹™** (Translate): è‹±æ–‡ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡
- **æ‘˜è¦ä»»å‹™** (Summarize): æœƒè­°è¨˜éŒ„æ‘˜è¦

### æ¸¬è©¦æ¨¡å‹æ¸…å–®
"""
        
        for i, model in enumerate(OLLAMA_MODELS_TO_COMPARE, 1):
            content += f"{i}. `{model}`\n"
        
        content += f"\n### è©•å¯©æ¨¡å‹\n"
        for reviewer_type, reviewer_model in REVIEWER_MODELS.items():
            if reviewer_model:
                content += f"- **{reviewer_type.upper()}**: `{reviewer_model}`\n"
        
        # è©•åˆ†è¡¨æ ¼
        for reviewer_type, reviewer_model in REVIEWER_MODELS.items():
            if reviewer_model is None or reviewer_type not in self.evaluation_scores:
                continue
                
            content += f"\n## {reviewer_type.upper()} è©•å¯©çµæœ\n\n"
            content += "| æ¨¡å‹ | ç¿»è­¯åˆ†æ•¸ | ç¿»è­¯è©•èª | æ‘˜è¦åˆ†æ•¸ | æ‘˜è¦è©•èª | å¹³å‡åˆ†æ•¸ |\n"
            content += "|------|----------|----------|----------|----------|----------|\n"
            
            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_type]:
                    translate_score = self.evaluation_scores[reviewer_type][model].get('translate', {}).get('score', 0)
                    translate_comment = self.evaluation_scores[reviewer_type][model].get('translate', {}).get('comment', 'N/A')
                    summarize_score = self.evaluation_scores[reviewer_type][model].get('summarize', {}).get('score', 0)
                    summarize_comment = self.evaluation_scores[reviewer_type][model].get('summarize', {}).get('comment', 'N/A')
                    avg_score = (translate_score + summarize_score) / 2
                    
                    content += f"| `{model}` | {translate_score} | {translate_comment} | {summarize_score} | {summarize_comment} | {avg_score:.1f} |\n"
        
        # çµ±è¨ˆåˆ†æ
        content += "\n## çµ±è¨ˆåˆ†æ\n\n"
        
        for reviewer_type in REVIEWER_MODELS.keys():
            if reviewer_type not in self.evaluation_scores:
                continue
                
            content += f"### {reviewer_type.upper()} è©•å¯©çµ±è¨ˆ\n\n"
            
            # è¨ˆç®—å„ä»»å‹™å¹³å‡åˆ†æ•¸
            translate_scores = []
            summarize_scores = []
            
            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_type]:
                    t_score = self.evaluation_scores[reviewer_type][model].get('translate', {}).get('score', 0)
                    s_score = self.evaluation_scores[reviewer_type][model].get('summarize', {}).get('score', 0)
                    if t_score > 0:
                        translate_scores.append(t_score)
                    if s_score > 0:
                        summarize_scores.append(s_score)
            
            if translate_scores:
                content += f"- **ç¿»è­¯ä»»å‹™å¹³å‡åˆ†æ•¸**: {np.mean(translate_scores):.2f}\n"
                content += f"- **ç¿»è­¯ä»»å‹™æœ€é«˜åˆ†**: {max(translate_scores)}\n"
                content += f"- **ç¿»è­¯ä»»å‹™æœ€ä½åˆ†**: {min(translate_scores)}\n"
            
            if summarize_scores:
                content += f"- **æ‘˜è¦ä»»å‹™å¹³å‡åˆ†æ•¸**: {np.mean(summarize_scores):.2f}\n"
                content += f"- **æ‘˜è¦ä»»å‹™æœ€é«˜åˆ†**: {max(summarize_scores)}\n"
                content += f"- **æ‘˜è¦ä»»å‹™æœ€ä½åˆ†**: {min(summarize_scores)}\n"
            
            content += "\n"
        
        # è¦–è¦ºåŒ–åœ–è¡¨
        content += "## è¦–è¦ºåŒ–åœ–è¡¨\n\n"
        
        for reviewer_type in REVIEWER_MODELS.keys():
            if reviewer_type not in self.evaluation_scores:
                continue
                
            chart_path = f"chart_{reviewer_type}.png"
            content += f"### {reviewer_type.upper()} è©•å¯©çµæœåœ–è¡¨\n\n"
            content += f"![{reviewer_type.upper()} è©•å¯©çµæœ]({chart_path})\n\n"
        
        # æ¨¡å‹è¼¸å‡ºçµæœ
        content += "## æ¨¡å‹è¼¸å‡ºçµæœ\n\n"
        
        for model in OLLAMA_MODELS_TO_COMPARE:
            content += f"### {model}\n\n"
            
            for task in SUPPORTED_TASKS.keys():
                content += f"#### {task.capitalize()} çµæœ\n\n"
                content += "```\n"
                content += self.results[model][task]
                content += "\n```\n\n"
        
        return content

    def create_charts(self):
        """ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨"""
        print("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆåœ–è¡¨...")
        
        for reviewer_type in REVIEWER_MODELS.keys():
            if reviewer_type not in self.evaluation_scores:
                continue
                
            # æº–å‚™æ•¸æ“š
            models = []
            translate_scores = []
            summarize_scores = []
            
            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_type]:
                    models.append(model.replace('hf.co/mradermacher/', '').replace(':Q4_K_M', ''))
                    translate_scores.append(self.evaluation_scores[reviewer_type][model].get('translate', {}).get('score', 0))
                    summarize_scores.append(self.evaluation_scores[reviewer_type][model].get('summarize', {}).get('score', 0))
            
            if not models:
                continue
                
            # å»ºç«‹é•·æ¢åœ–
            x = np.arange(len(models))
            width = 0.35
            
            fig, ax = plt.subplots(figsize=(12, 8))
            bars1 = ax.bar(x - width/2, translate_scores, width, label='ç¿»è­¯ (Translate)', alpha=0.8)
            bars2 = ax.bar(x + width/2, summarize_scores, width, label='æ‘˜è¦ (Summarize)', alpha=0.8)
            
            ax.set_xlabel('æ¨¡å‹')
            ax.set_ylabel('åˆ†æ•¸')
            ax.set_title(f'æ¨¡å‹è©•æ¯”çµæœ - {reviewer_type.upper()} è©•å¯©')
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45, ha='right')
            ax.legend()
            ax.set_ylim(0, 10)
            
            # åœ¨é•·æ¢ä¸Šé¡¯ç¤ºæ•¸å€¼
            def autolabel(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax.annotate(f'{height}',
                               xy=(bar.get_x() + bar.get_width() / 2, height),
                               xytext=(0, 3),
                               textcoords="offset points",
                               ha='center', va='bottom')
            
            autolabel(bars1)
            autolabel(bars2)
            
            plt.tight_layout()
            chart_path = f"reports/chart_{reviewer_type}.png"
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"âœ… åœ–è¡¨å·²ç”Ÿæˆ: {chart_path}")

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print("ğŸ¯ Ollama æ¨¡å‹è©•æ¯”ç³»çµ±")
    print("=" * 50)
    
    # æª¢æŸ¥è¨­å®š
    if not OLLAMA_MODELS_TO_COMPARE:
        print("âŒ æœªè¨­å®šè¦è©•æ¯”çš„æ¨¡å‹ï¼Œè«‹æª¢æŸ¥ config.py")
        sys.exit(1)
    
    if not any(REVIEWER_MODELS.values()):
        print("âŒ æœªè¨­å®šè©•å¯©æ¨¡å‹ï¼Œè«‹æª¢æŸ¥ config.py")
        sys.exit(1)
    
    # å»ºç«‹è©•æ¯”å™¨ä¸¦åŸ·è¡Œ
    evaluator = ModelEvaluator()
    
    try:
        # åŸ·è¡Œè©•æ¯”
        evaluator.run_evaluation()
        
        # ç”Ÿæˆå ±è¡¨
        md_path, html_path = evaluator.generate_report()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ è©•æ¯”å®Œæˆï¼")
        print(f"ğŸ“„ Markdown å ±è¡¨: {md_path}")
        print(f"ğŸŒ HTML å ±è¡¨: {html_path}")
        print("=" * 50)
        
    except KeyboardInterrupt:
        print("\nâŒ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 