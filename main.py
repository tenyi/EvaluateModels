#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import re
import sys
import time
from openai import OpenAI
import requests
from typing import Tuple
from datetime import datetime
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import numpy as np
from config import (
    OLLAMA_API_BASE_URL,
    OLLAMA_MODELS_TO_COMPARE,
    OPENAI_API_KEY,
    GOOGLE_API_KEY,
    OPENROUTER_API_KEY,  # Added
    REPLICATE_API_KEY,  # Added
    REVIEWER_MODELS,
    REVIEWER_TEMPERATURE,  # Added
    SUPPORTED_TASKS,
)
from markdown2html import convert_markdown_to_html
from cache_utils import get_cache_key, load_from_cache, save_to_cache  # Added

# è¨­å®šä¸­æ–‡å­—é«”
plt.rcParams["font.sans-serif"] = ["Arial Unicode MS", "SimHei", "DejaVu Sans"]
plt.rcParams["axes.unicode_minus"] = False


class ModelEvaluator:
    def __init__(self):
        self.results = {}
        self.evaluation_scores = {}

    def read_input_text(self, file_path: str = "input.txt") -> str:
        """è®€å–æ¸¬è©¦æ¨£æœ¬æ–‡ä»¶"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {file_path}")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            sys.exit(1)

    def call_ollama_api(self, model: str, task: str, text: str) -> str:
        """å‘¼å« Ollama API"""
        cache_params = {
            "provider": "ollama",
            "model": model,
            "task": task,
            "text": text,
        }
        cache_key = get_cache_key(cache_params)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… {model} ({task}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        url = f"{OLLAMA_API_BASE_URL}/api/chat"
        headers = {"Content-Type": "application/json"}

        prompt = SUPPORTED_TASKS[task]

        data = {
            "model": model,
            "messages": [
                {"role": "system", "content": prompt},
                {"role": "user", "content": text},
            ],
            "stream": False,
        }

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« {model} åŸ·è¡Œ {task} ä»»å‹™...")
            response = requests.post(url, headers=headers, json=data, timeout=120)
            response.raise_for_status()

            result = response.json()
            output_text = result["message"]["content"].strip()
            output_text = re.sub(
                r"<think>.*?</think>", "", output_text, flags=re.DOTALL
            )
            save_to_cache(cache_key, output_text)
            return output_text

        except requests.exceptions.Timeout:
            print(f"  âš ï¸  {model} å›æ‡‰è¶…æ™‚")
            return "ERROR: å›æ‡‰è¶…æ™‚"
        except requests.exceptions.RequestException as e:
            print(f"  âŒ {model} API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: API å‘¼å«å¤±æ•— - {e}"
        except Exception as e:
            print(f"  âŒ {model} è™•ç†å›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ERROR: è™•ç†å›æ‡‰å¤±æ•— - {e}"

    def call_openai_api(
        self,
        model: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """å‘¼å« OpenAI API"""
        print(f"is_reviewer: {is_reviewer}")
        cache_params = {
            "provider": "openai",
            "model": model,
            "system_prompt": system_prompt,
            "user_content": user_content,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… OpenAI ({model}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
            return "ERROR: OpenAI API é‡‘é‘°æœªè¨­å®š"

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« OpenAI API ({model})...")

            # å–å¾— temperature è¨­å®š
            temperature = REVIEWER_TEMPERATURE.get("openai", 0.1)

            # å»ºç«‹è«‹æ±‚åƒæ•¸
            request_params = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content},
                ],
                "timeout": 60,
            }

            # åªæœ‰ç•¶ temperature ä¸ç‚º 1 æ™‚æ‰åŠ å…¥åƒæ•¸ï¼ˆæŸäº›æ¨¡å‹å¦‚ O3/O4 ä¸å…è¨±æŒ‡å®š temperatureï¼‰
            if temperature is not None and temperature != 1:
                request_params["temperature"] = temperature

            with OpenAI(api_key=OPENAI_API_KEY) as client:
                response = client.chat.completions.create(**request_params)

                output_text = response.choices[0].message.content.strip()
                save_to_cache(cache_key, output_text)
                return output_text

        except Exception as e:
            print(f"  âŒ OpenAI API è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ERROR: OpenAI API è™•ç†å¤±æ•— - {e}"

    # def call_openai_api(self, model: str, system_prompt: str, user_content: str, is_reviewer: bool = False) -> str:
    #     """å‘¼å« OpenAI API"""
    #     cache_params = {"provider": "openai", "model": model, "system_prompt": system_prompt, "user_content": user_content, "is_reviewer": is_reviewer}
    #     cache_key = get_cache_key(cache_params)
    #     cached_response = load_from_cache(cache_key)
    #     if cached_response:
    #         print(f"  âœ… OpenAI ({model}) å¾å¿«å–è¼‰å…¥")
    #         return cached_response

    #     if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
    #         return "ERROR: OpenAI API é‡‘é‘°æœªè¨­å®š"

    #     url = "https://api.openai.com/v1/chat/completions"
    #     headers = {
    #         "Content-Type": "application/json",
    #         "Authorization": f"Bearer {OPENAI_API_KEY}"
    #     }

    #     data = {
    #         "model": model,
    #         "messages": [
    #             {"role": "system", "content": system_prompt},
    #             {"role": "user", "content": user_content}
    #         ],
    #         "temperature": 0.1
    #     }

    #     try:
    #         print(f"  ğŸ”„ æ­£åœ¨å‘¼å« OpenAI API ({model})...")
    #         response = requests.post(url, headers=headers, json=data, timeout=60)
    #         response.raise_for_status()

    #         result = response.json()
    #         output_text = result["choices"][0]["message"]["content"].strip()
    #         save_to_cache(cache_key, output_text)
    #         return output_text

    #     except requests.exceptions.Timeout:
    #         print(f"  âš ï¸  OpenAI API å›æ‡‰è¶…æ™‚")
    #         return "ERROR: OpenAI API å›æ‡‰è¶…æ™‚"
    #     except requests.exceptions.RequestException as e:
    #         print(f"  âŒ OpenAI API å‘¼å«å¤±æ•—: {e}")
    #         return f"ERROR: OpenAI API å‘¼å«å¤±æ•— - {e}"
    #     except KeyError as e:
    #         print(f"  âŒ OpenAI API å›æ‡‰æ ¼å¼éŒ¯èª¤: {e}")
    #         return f"ERROR: OpenAI API å›æ‡‰æ ¼å¼éŒ¯èª¤ - {e}"
    #     except Exception as e:
    #         print(f"  âŒ OpenAI API è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    #         return f"ERROR: OpenAI API è™•ç†å¤±æ•— - {e}"

    def call_google_api(
        self,
        model: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """å‘¼å« Google Gemini API"""
        cache_params = {
            "provider": "google",
            "model": model,
            "system_prompt": system_prompt,
            "user_content": user_content,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… Google ({model}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        if not GOOGLE_API_KEY or GOOGLE_API_KEY == "your_google_api_key_here":
            return "ERROR: Google API é‡‘é‘°æœªè¨­å®š"

        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={GOOGLE_API_KEY}"
        headers = {"Content-Type": "application/json"}

        # å–å¾— temperature è¨­å®š
        temperature = REVIEWER_TEMPERATURE.get("gemini", 0.1)

        data = {
            "contents": [{"parts": [{"text": f"{system_prompt}\n\n{user_content}"}]}]
        }

        # åªæœ‰ç•¶ temperature ä¸ç‚º 1 æ™‚æ‰åŠ å…¥ generationConfigï¼ˆæŸäº›æ¨¡å‹ä¸å…è¨±æŒ‡å®š temperatureï¼‰
        if temperature is not None and temperature != 1:
            data["generationConfig"] = {"temperature": temperature}

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« Google API ({model})...")
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()

            result = response.json()
            output_text = result["candidates"][0]["content"]["parts"][0]["text"].strip()
            save_to_cache(cache_key, output_text)
            return output_text

        except Exception as e:
            print(f"  âŒ Google API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: Google API å¤±æ•— - {e}"

    def call_openrouter_api(
        self,
        model: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """å‘¼å« OpenRouter API"""
        cache_params = {
            "provider": "openrouter",
            "model": model,
            "system_prompt": system_prompt,
            "user_content": user_content,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… OpenRouter ({model}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        if (
            not OPENROUTER_API_KEY
            or OPENROUTER_API_KEY == "your_openrouter_api_key_here"
        ):
            return "ERROR: OpenRouter API é‡‘é‘°æœªè¨­å®š"

        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENROUTER_API_KEY}",
            "HTTP-Referer": "http://localhost",  # Optional, for analytics
            "X-Title": "Ollama Model Evaluator",  # Optional, for analytics
        }

        # å–å¾— temperature è¨­å®š
        temperature = REVIEWER_TEMPERATURE.get("openrouter", 0.1)

        data = {
            "model": model,  # e.g., "mistralai/mistral-7b-instruct"
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content},
            ],
        }

        # åªæœ‰ç•¶ temperature ä¸ç‚º 1 æ™‚æ‰åŠ å…¥åƒæ•¸ï¼ˆæŸäº›æ¨¡å‹ä¸å…è¨±æŒ‡å®š temperatureï¼‰
        if temperature is not None and temperature != 1:
            data["temperature"] = temperature

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« OpenRouter API ({model})...")
            response = requests.post(url, headers=headers, json=data, timeout=60)
            response.raise_for_status()

            result = response.json()
            output_text = result["choices"][0]["message"]["content"].strip()
            save_to_cache(cache_key, output_text)
            return output_text
        except Exception as e:
            print(f"  âŒ OpenRouter API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: OpenRouter API å¤±æ•— - {e}"

    def call_replicate_api(
        self,
        model_version: str,
        system_prompt: str,
        user_content: str,
        is_reviewer: bool = False,
    ) -> str:
        """å‘¼å« Replicate API"""
        # model_version is typically in "owner/model_name:version_hash" format
        cache_params = {
            "provider": "replicate",
            "model_version": model_version,
            "system_prompt": system_prompt,
            "user_content": user_content,
            "is_reviewer": is_reviewer,
        }
        cache_key = get_cache_key(cache_params)
        cached_response = load_from_cache(cache_key)
        if cached_response:
            print(f"  âœ… Replicate ({model_version}) å¾å¿«å–è¼‰å…¥")
            return cached_response

        if not REPLICATE_API_KEY or REPLICATE_API_KEY == "your_replicate_api_key_here":
            return "ERROR: Replicate API é‡‘é‘°æœªè¨­å®š"

        url = "https://api.replicate.com/v1/predictions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Token {REPLICATE_API_KEY}",
        }

        # Constructing input based on common Replicate model patterns.
        # This might need adjustment based on the specific model.
        # Typically, Replicate models take a prompt or a structured input.
        # For chat-like models, combining system and user prompt is common.
        full_prompt = f"{system_prompt}\n\nUser: {user_content}\nAssistant:"

        # å–å¾— temperature è¨­å®š
        temperature = REVIEWER_TEMPERATURE.get("replicate", 0.1)

        input_data = {
            "prompt": full_prompt,
            "system_prompt": system_prompt,  # Some models might use this explicitly
            "max_new_tokens": 1024,  # Example, adjust
        }

        # åªæœ‰ç•¶ temperature ä¸ç‚º 1 æ™‚æ‰åŠ å…¥åƒæ•¸ï¼ˆæŸäº›æ¨¡å‹ä¸å…è¨±æŒ‡å®š temperatureï¼‰
        if temperature is not None and temperature != 1:
            input_data["temperature"] = temperature

        data = {
            "version": model_version.split(":")[-1]
            if ":" in model_version
            else model_version,  # Expects version hash
            "input": input_data,
        }

        try:
            print(f"  ğŸ”„ æ­£åœ¨å‘¼å« Replicate API ({model_version})...")
            # Start prediction
            response = requests.post(url, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            prediction_start_result = response.json()

            prediction_id = prediction_start_result.get("id")
            if not prediction_id:
                return f"ERROR: Replicate API æœªè¿”å›æœ‰æ•ˆçš„ prediction ID - {prediction_start_result.get('detail', 'ç„¡è©³ç´°éŒ¯èª¤')}"

            # Poll for result
            prediction_url: str = f"https://api.replicate.com/v1/predictions/{prediction_id}"
            max_retries = 20  # ~100 seconds
            retry_interval = 5  # seconds

            for _ in range(max_retries):
                time.sleep(retry_interval)
                poll_response = requests.get(
                    prediction_url, headers=headers, timeout=30
                )
                poll_response.raise_for_status()
                poll_result = poll_response.json()

                status = poll_result.get("status")
                if status == "succeeded":
                    # Output format varies; often it's a list of strings or a single string.
                    # Adapting for a common case where output is a list of strings (joined).
                    output = poll_result.get("output")
                    if isinstance(output, list):
                        output_text = "".join(output).strip()
                    elif isinstance(output, str):
                        output_text = output.strip()
                    else:
                        return (
                            f"ERROR: Replicate API è¿”å›äº†æœªçŸ¥çš„è¼¸å‡ºæ ¼å¼: {type(output)}"
                        )

                    save_to_cache(cache_key, output_text)
                    return output_text
                elif status == "failed" or status == "canceled":
                    error_detail = poll_result.get("error", "æœªçŸ¥éŒ¯èª¤")
                    return f"ERROR: Replicate é æ¸¬å¤±æ•—æˆ–å–æ¶ˆ - {error_detail}"
                # Continue polling if status is "starting" or "processing"

            return f"ERROR: Replicate é æ¸¬è¶…æ™‚ ({model_version})"

        except requests.exceptions.RequestException as e:
            print(f"  âŒ Replicate API å‘¼å«å¤±æ•—: {e}")
            return f"ERROR: Replicate API è«‹æ±‚å¤±æ•— - {e}"
        except Exception as e:
            print(f"  âŒ Replicate API è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"ERROR: Replicate API éŒ¯èª¤ - {e}"

    def evaluate_with_reviewer(
        self,
        reviewer_type: str,
        reviewer_model: str,
        task: str,
        original_text: str,
        model_output: str,
    ) -> Tuple[int, str]:
        """ä½¿ç”¨è©•å¯©æ¨¡å‹è©•åˆ†"""

        if task == "translate":
            system_prompt = """ä½ æ˜¯å°ˆæ¥­çš„ç¿»è­¯è©•å¯©å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–å°ç¿»è­¯çµæœè©•åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š

è©•åˆ†æ¨™æº–ï¼š
- é€šé †æ€§ï¼ˆ1-3åˆ†ï¼‰ï¼šç¿»è­¯æ˜¯å¦è‡ªç„¶æµæš¢ï¼Œç¬¦åˆä¸­æ–‡è¡¨é”ç¿’æ…£
- æº–ç¢ºæ€§ï¼ˆ1-3åˆ†ï¼‰ï¼šæ˜¯å¦æœ‰ç¿»è­¯éŒ¯èª¤ã€éºæ¼æˆ–èª¤è§£
- éµå¾ªæŒ‡ä»¤(1-2åˆ†)ï¼šæ˜¯å¦å®Œå…¨éµå¾ªæŒ‡ä»¤ï¼Œä»¥ç¹é«”ä¸­æ–‡å›è¦†
- å°ˆæ¥­è¡“èªè™•ç†ï¼ˆ1-2åˆ†ï¼‰ï¼šè‹±æ–‡å°ˆæ¥­è¡“èªæ˜¯å¦é©ç•¶ä¿ç•™

è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š
åˆ†æ•¸: [1-10çš„æ•´æ•¸]
è©•èª: [ç°¡çŸ­è©•èªï¼Œèªªæ˜è©•åˆ†ç†ç”±]"""

            user_content = f"""åŸæ–‡ï¼š
{original_text}

ç¿»è­¯çµæœï¼š
{model_output}

è«‹è©•åˆ†ä¸¦çµ¦å‡ºè©•èªã€‚"""

        elif task == "summarize":
            system_prompt = """ä½ æ˜¯å°ˆæ¥­çš„æ‘˜è¦è©•å¯©å°ˆå®¶ã€‚è«‹æ ¹æ“šä»¥ä¸‹æ¨™æº–å°æ‘˜è¦çµæœè©•åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š

è©•åˆ†æ¨™æº–ï¼š
- é‡é»æ¶µè“‹ï¼ˆ1-3åˆ†ï¼‰ï¼šé‡è¦è­°é¡Œå’Œé—œéµæˆæœæ˜¯å¦æœ‰æåŠ
- è¡¨é”æ¸…æ¥šï¼ˆ1-3åˆ†ï¼‰ï¼šæ‘˜è¦æ˜¯å¦æ¢ç†åˆ†æ˜ã€æ˜“æ–¼ç†è§£  
- éµå¾ªæŒ‡ä»¤(1-2åˆ†)ï¼šæ˜¯å¦å®Œå…¨éµå¾ªæŒ‡ä»¤ï¼Œä»¥ç¹é«”ä¸­æ–‡å›è¦†
- ç°¡æ½”æ€§ï¼ˆ1-2åˆ†ï¼‰ï¼šæ˜¯å¦é¿å…å†—é¤˜ï¼Œåˆ‡ä¸­è¦é»

è«‹ä»¥ä»¥ä¸‹æ ¼å¼å›è¦†ï¼š
åˆ†æ•¸: [1-10çš„æ•´æ•¸]
è©•èª: [ç°¡çŸ­è©•èªï¼Œèªªæ˜è©•åˆ†ç†ç”±]"""

            user_content = f"""åŸæ–‡ï¼š
{original_text}

æ‘˜è¦çµæœï¼š
{model_output}

è«‹è©•åˆ†ä¸¦çµ¦å‡ºè©•èªã€‚"""

        # å‘¼å«å°æ‡‰çš„è©•å¯© API
        if reviewer_type == "openai":
            response = self.call_openai_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        elif reviewer_type == "gemini":
            response = self.call_google_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        elif reviewer_type == "openrouter":
            response = self.call_openrouter_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        elif reviewer_type == "replicate":
            # Replicate API needs the model version, which is reviewer_model here
            response = self.call_replicate_api(
                reviewer_model, system_prompt, user_content, is_reviewer=True
            )
        else:
            return 0, "ERROR: ä¸æ”¯æ´çš„è©•å¯©æ¨¡å‹é¡å‹"

        # è§£æè©•åˆ†çµæœ
        try:
            lines = response.split("\n")
            score = 0
            comment = "ç„¡è©•èª"

            for line in lines:
                if line.startswith("åˆ†æ•¸:") or line.startswith("åˆ†æ•°:"):
                    score_text = line.split(":")[1].strip()
                    score = int("".join(filter(str.isdigit, score_text)))
                elif line.startswith("è©•èª:") or line.startswith("è¯„è¯­:"):
                    comment = line.split(":", 1)[1].strip()

            # ç¢ºä¿åˆ†æ•¸åœ¨æœ‰æ•ˆç¯„åœå…§
            score = max(1, min(10, score))
            return score, comment

        except Exception as e:
            print(f"  âš ï¸  è§£æè©•åˆ†å¤±æ•—: {e}")
            return 5, f"è§£æå¤±æ•—: {response[:100]}..."

    def run_evaluation(self):
        """åŸ·è¡Œå®Œæ•´çš„è©•æ¯”æµç¨‹"""
        print("ğŸš€ é–‹å§‹æ¨¡å‹è©•æ¯”...")

        # è®€å–æ¸¬è©¦æ–‡æœ¬
        input_text = self.read_input_text()
        print(f"ğŸ“– å·²è®€å–æ¸¬è©¦æ–‡æœ¬ ({len(input_text)} å­—å…ƒ)")

        # å°æ¯å€‹æ¨¡å‹åŸ·è¡Œä»»å‹™
        for model in OLLAMA_MODELS_TO_COMPARE:
            print(f"\nğŸ” æ­£åœ¨æ¸¬è©¦æ¨¡å‹: {model}")
            self.results[model] = {}

            for task in SUPPORTED_TASKS.keys():
                print(f"  ğŸ“ åŸ·è¡Œä»»å‹™: {task}")

                # å‘¼å« Ollama æ¨¡å‹
                result = self.call_ollama_api(model, task, input_text)
                self.results[model][task] = result

                if result.startswith("ERROR:"):
                    print(f"  âŒ ä»»å‹™å¤±æ•—: {result}")
                else:
                    print(f"  âœ… ä»»å‹™å®Œæˆ ({len(result)} å­—å…ƒ)")

                # çŸ­æš«å»¶é²é¿å… API é™åˆ¶
                time.sleep(1)

        # ä½¿ç”¨è©•å¯©æ¨¡å‹è©•åˆ†
        print(f"\nâš–ï¸  é–‹å§‹è©•å¯©éšæ®µ...")
        self.evaluation_scores = {}

        for reviewer_type, reviewer_model in REVIEWER_MODELS.items():
            if reviewer_model is None:
                continue

            print(f"\nğŸ¯ ä½¿ç”¨è©•å¯©æ¨¡å‹: {reviewer_type} ({reviewer_model})")
            self.evaluation_scores[reviewer_type] = {}

            for model in OLLAMA_MODELS_TO_COMPARE:
                self.evaluation_scores[reviewer_type][model] = {}

                for task in SUPPORTED_TASKS.keys():
                    if self.results[model][task].startswith("ERROR:"):
                        score, comment = 0, "æ¨¡å‹åŸ·è¡Œå¤±æ•—"
                    else:
                        print(f"  ğŸ“Š è©•å¯© {model} çš„ {task} çµæœ...")
                        score, comment = self.evaluate_with_reviewer(
                            reviewer_type,
                            reviewer_model,
                            task,
                            input_text,
                            self.results[model][task],
                        )

                    self.evaluation_scores[reviewer_type][model][task] = {
                        "score": score,
                        "comment": comment,
                    }

                    print(f"    åˆ†æ•¸: {score}/10")
                    time.sleep(1)  # é¿å… API é™åˆ¶

    def generate_report(self):
        """ç”Ÿæˆè©•æ¯”å ±è¡¨"""
        print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆå ±è¡¨...")

        # å…ˆç”Ÿæˆåœ–è¡¨
        self.create_charts()

        # å»ºç«‹å ±è¡¨å…§å®¹
        report_content = self.create_markdown_report()

        # å¯«å…¥ Markdown æª”æ¡ˆ
        report_md_path = "reports/evaluation_report.md"
        with open(report_md_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"âœ… Markdown å ±è¡¨å·²ç”Ÿæˆ: {report_md_path}")

        # è½‰æ›ç‚º HTML
        report_html_path = "reports/evaluation_report.html"
        convert_markdown_to_html(report_md_path, report_html_path)
        print(f"âœ… HTML å ±è¡¨å·²ç”Ÿæˆ: {report_html_path}")

        return report_md_path, report_html_path

    def create_markdown_report(self) -> str:
        """å»ºç«‹ Markdown æ ¼å¼çš„å ±è¡¨"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        content = f"""# Ollama æ¨¡å‹è©•æ¯”å ±è¡¨

**ç”Ÿæˆæ™‚é–“**: {current_time}

## è©•æ¯”æ¦‚è¦

æœ¬æ¬¡è©•æ¯”æ¸¬è©¦äº† {len(OLLAMA_MODELS_TO_COMPARE)} å€‹ Ollama æ¨¡å‹åœ¨å…©å€‹ä»»å‹™ä¸Šçš„è¡¨ç¾ï¼š
- **ç¿»è­¯ä»»å‹™** (Translate): è‹±æ–‡ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡
- **æ‘˜è¦ä»»å‹™** (Summarize): æœƒè­°è¨˜éŒ„æ‘˜è¦

### æ¸¬è©¦æ¨¡å‹æ¸…å–®
"""

        for i, model in enumerate(OLLAMA_MODELS_TO_COMPARE, 1):
            content += f"{i}. `{model}`\n"

        content += f"\n### è©•å¯©æ¨¡å‹\n"
        for reviewer_type, reviewer_model in REVIEWER_MODELS.items():
            if reviewer_model:
                content += f"- **{reviewer_type.upper()}**: `{reviewer_model}`\n"

        # è©•åˆ†è¡¨æ ¼
        for reviewer_type, reviewer_model in REVIEWER_MODELS.items():
            if reviewer_model is None or reviewer_type not in self.evaluation_scores:
                continue

            content += f"\n## {reviewer_type.upper()} è©•å¯©çµæœ\n\n"
            content += (
                "| æ¨¡å‹ | ç¿»è­¯åˆ†æ•¸ | ç¿»è­¯è©•èª | æ‘˜è¦åˆ†æ•¸ | æ‘˜è¦è©•èª | å¹³å‡åˆ†æ•¸ |\n"
            )
            content += (
                "|------|----------|----------|----------|----------|----------|\n"
            )

            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_type]:
                    translate_score = (
                        self.evaluation_scores[reviewer_type][model]
                        .get("translate", {})
                        .get("score", 0)
                    )
                    translate_comment = (
                        self.evaluation_scores[reviewer_type][model]
                        .get("translate", {})
                        .get("comment", "N/A")
                    )
                    summarize_score = (
                        self.evaluation_scores[reviewer_type][model]
                        .get("summarize", {})
                        .get("score", 0)
                    )
                    summarize_comment = (
                        self.evaluation_scores[reviewer_type][model]
                        .get("summarize", {})
                        .get("comment", "N/A")
                    )
                    avg_score = (translate_score + summarize_score) / 2

                    content += f"| `{model}` | {translate_score} | {translate_comment} | {summarize_score} | {summarize_comment} | {avg_score:.1f} |\n"

        # çµ±è¨ˆåˆ†æ
        content += "\n## çµ±è¨ˆåˆ†æ\n\n"

        for reviewer_type in REVIEWER_MODELS.keys():
            if reviewer_type not in self.evaluation_scores:
                continue

            content += f"### {reviewer_type.upper()} è©•å¯©çµ±è¨ˆ\n\n"

            # è¨ˆç®—å„ä»»å‹™å¹³å‡åˆ†æ•¸
            translate_scores = []
            summarize_scores = []

            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_type]:
                    t_score = (
                        self.evaluation_scores[reviewer_type][model]
                        .get("translate", {})
                        .get("score", 0)
                    )
                    s_score = (
                        self.evaluation_scores[reviewer_type][model]
                        .get("summarize", {})
                        .get("score", 0)
                    )
                    if t_score > 0:
                        translate_scores.append(t_score)
                    if s_score > 0:
                        summarize_scores.append(s_score)

            if translate_scores:
                content += f"- **ç¿»è­¯ä»»å‹™å¹³å‡åˆ†æ•¸**: {np.mean(translate_scores):.2f}\n"
                content += f"- **ç¿»è­¯ä»»å‹™æœ€é«˜åˆ†**: {max(translate_scores)}\n"
                content += f"- **ç¿»è­¯ä»»å‹™æœ€ä½åˆ†**: {min(translate_scores)}\n"

            if summarize_scores:
                content += f"- **æ‘˜è¦ä»»å‹™å¹³å‡åˆ†æ•¸**: {np.mean(summarize_scores):.2f}\n"
                content += f"- **æ‘˜è¦ä»»å‹™æœ€é«˜åˆ†**: {max(summarize_scores)}\n"
                content += f"- **æ‘˜è¦ä»»å‹™æœ€ä½åˆ†**: {min(summarize_scores)}\n"

            content += "\n"

        # è¦–è¦ºåŒ–åœ–è¡¨
        content += "## è¦–è¦ºåŒ–åœ–è¡¨\n\n"

        for reviewer_type in REVIEWER_MODELS.keys():
            if reviewer_type not in self.evaluation_scores:
                continue

            chart_path = f"chart_{reviewer_type}.png"
            content += f"### {reviewer_type.upper()} è©•å¯©çµæœåœ–è¡¨\n\n"
            content += f"![{reviewer_type.upper()} è©•å¯©çµæœ]({chart_path})\n\n"

        # æ¨¡å‹è¼¸å‡ºçµæœ
        content += "## æ¨¡å‹è¼¸å‡ºçµæœ\n\n"

        for model in OLLAMA_MODELS_TO_COMPARE:
            content += f"### {model}\n\n"

            for task in SUPPORTED_TASKS.keys():
                content += f"#### {task.capitalize()} çµæœ\n\n"
                content += "```\n"
                content += self.results[model][task]
                content += "\n```\n\n"

        return content

    def create_charts(self):
        """ç”Ÿæˆæ¯”è¼ƒåœ–è¡¨"""
        print("ğŸ“ˆ æ­£åœ¨ç”Ÿæˆåœ–è¡¨...")

        for reviewer_type in REVIEWER_MODELS.keys():
            if reviewer_type not in self.evaluation_scores:
                continue

            # æº–å‚™æ•¸æ“š
            models = []
            translate_scores = []
            summarize_scores = []

            for model in OLLAMA_MODELS_TO_COMPARE:
                if model in self.evaluation_scores[reviewer_type]:
                    models.append(
                        model.replace("hf.co/mradermacher/", "").replace(":Q4_K_M", "")
                    )
                    translate_scores.append(
                        self.evaluation_scores[reviewer_type][model]
                        .get("translate", {})
                        .get("score", 0)
                    )
                    summarize_scores.append(
                        self.evaluation_scores[reviewer_type][model]
                        .get("summarize", {})
                        .get("score", 0)
                    )

            if not models:
                continue

            # å»ºç«‹é•·æ¢åœ–
            x = np.arange(len(models))
            width = 0.35

            fig, ax = plt.subplots(figsize=(12, 8))
            bars1 = ax.bar(
                x - width / 2,
                translate_scores,
                width,
                label="ç¿»è­¯ (Translate)",
                alpha=0.8,
            )
            bars2 = ax.bar(
                x + width / 2,
                summarize_scores,
                width,
                label="æ‘˜è¦ (Summarize)",
                alpha=0.8,
            )

            ax.set_xlabel("æ¨¡å‹")
            ax.set_ylabel("åˆ†æ•¸")
            ax.set_title(f"æ¨¡å‹è©•æ¯”çµæœ - {reviewer_type.upper()} è©•å¯©")
            ax.set_xticks(x)
            ax.set_xticklabels(models, rotation=45, ha="right")
            ax.legend()
            ax.set_ylim(0, 10)

            # åœ¨é•·æ¢ä¸Šé¡¯ç¤ºæ•¸å€¼
            def autolabel(bars):
                for bar in bars:
                    height = bar.get_height()
                    ax.annotate(
                        f"{height}",
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha="center",
                        va="bottom",
                    )

            autolabel(bars1)
            autolabel(bars2)

            plt.tight_layout()
            chart_path = f"reports/chart_{reviewer_type}.png"
            plt.savefig(chart_path, dpi=300, bbox_inches="tight")
            plt.close()

            print(f"âœ… åœ–è¡¨å·²ç”Ÿæˆ: {chart_path}")


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    print("ğŸ¯ Ollama æ¨¡å‹è©•æ¯”ç³»çµ±")
    print("=" * 50)

    # æª¢æŸ¥è¨­å®š
    if not OLLAMA_MODELS_TO_COMPARE:
        print("âŒ æœªè¨­å®šè¦è©•æ¯”çš„æ¨¡å‹ï¼Œè«‹æª¢æŸ¥ config.py")
        sys.exit(1)

    if not any(REVIEWER_MODELS.values()):
        print("âŒ æœªè¨­å®šè©•å¯©æ¨¡å‹ï¼Œè«‹æª¢æŸ¥ config.py")
        sys.exit(1)

    # å»ºç«‹è©•æ¯”å™¨ä¸¦åŸ·è¡Œ
    evaluator = ModelEvaluator()

    try:
        # åŸ·è¡Œè©•æ¯”
        evaluator.run_evaluation()

        # ç”Ÿæˆå ±è¡¨
        md_path, html_path = evaluator.generate_report()

        print("\n" + "=" * 50)
        print("ğŸ‰ è©•æ¯”å®Œæˆï¼")
        print(f"ğŸ“„ Markdown å ±è¡¨: {md_path}")
        print(f"ğŸŒ HTML å ±è¡¨: {html_path}")
        print("=" * 50)

    except KeyboardInterrupt:
        print("\nâŒ ä½¿ç”¨è€…ä¸­æ–·åŸ·è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
