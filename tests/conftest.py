#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
conftest.pyï¼špytest çš„è¨­å®šæª”

é€™å€‹æª”æ¡ˆåŒ…å«äº†æ•´å€‹æ¸¬è©¦å¥—ä»¶å…±äº«çš„ fixtures å’Œè¨­å®šã€‚
pytest æœƒè‡ªå‹•ç™¼ç¾ä¸¦ä½¿ç”¨é€™å€‹æª”æ¡ˆä¸­çš„è¨­å®šï¼Œç„¡éœ€æ‰‹å‹•åŒ¯å…¥ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- è¨­å®šå…±äº«çš„æ¸¬è©¦è³‡æº (fixtures)ï¼Œä¾‹å¦‚è‡¨æ™‚ç›®éŒ„ã€è¨­å®šæª”ã€æ¨¡æ“¬è³‡æ–™ç­‰ã€‚
- ä¿®æ”¹ pytest çš„è¡Œç‚ºï¼Œä¾‹å¦‚æ–°å¢è‡ªè¨‚æ¨™è¨˜ã€èª¿æ•´æ¸¬è©¦æ”¶é›†æµç¨‹ã€‚
- åœ¨æ¸¬è©¦æœƒè©±ï¼ˆsessionï¼‰çš„ä¸åŒéšæ®µåŸ·è¡Œç‰¹å®šç¨‹å¼ç¢¼ï¼Œä¾‹å¦‚é–‹å§‹å’ŒçµæŸæ™‚ã€‚
"""

# åŒ¯å…¥å¿…è¦çš„æ¨¡çµ„
import pytest  # pytest æ¸¬è©¦æ¡†æ¶
import os  # è™•ç†ä½œæ¥­ç³»çµ±ç›¸é—œåŠŸèƒ½ï¼Œå¦‚è·¯å¾‘
import sys  # å­˜å– Python ç›´è­¯å™¨çš„è®Šæ•¸å’Œå‡½å¼
import tempfile  # å»ºç«‹è‡¨æ™‚æª”æ¡ˆå’Œç›®éŒ„
import shutil  # æä¾›é«˜éšçš„æª”æ¡ˆæ“ä½œåŠŸèƒ½
from unittest.mock import patch  # ç”¨æ–¼æ¨¡æ“¬ (mock) ç‰©ä»¶å’Œå‡½å¼

# --- è·¯å¾‘è¨­å®š ---

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥ Python çš„æ¨¡çµ„æœå°‹è·¯å¾‘
# é€™æ¨£å¯ä»¥ç¢ºä¿åœ¨åŸ·è¡Œæ¸¬è©¦æ™‚ï¼Œå¯ä»¥æ­£ç¢ºåœ°åŒ¯å…¥å°ˆæ¡ˆä¸­çš„å…¶ä»–æ¨¡çµ„
# os.path.abspath(__file__) å–å¾—ç›®å‰æª”æ¡ˆçš„çµ•å°è·¯å¾‘
# os.path.dirname() å–å¾—ç›®éŒ„åç¨±
# sys.path.insert(0, ...) å°‡è·¯å¾‘æ’å…¥åˆ°æœå°‹è·¯å¾‘çš„æœ€å‰é¢ï¼Œå„ªå…ˆè¢«æœå°‹
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


# --- Fixtures: æ¸¬è©¦è³‡æºè¨­å®š ---

@pytest.fixture(scope="session")
def test_data_dir():
    """
    å»ºç«‹ä¸€å€‹ session ç­‰ç´šçš„è‡¨æ™‚ç›®éŒ„ï¼Œç”¨æ–¼å­˜æ”¾æ¸¬è©¦æ•¸æ“šã€‚
    'scope="session"' è¡¨ç¤ºé€™å€‹ fixture åœ¨æ•´å€‹æ¸¬è©¦æœƒè©±ä¸­åªæœƒåŸ·è¡Œä¸€æ¬¡ã€‚
    
    ä½¿ç”¨ `yield` å°‡ç›®éŒ„è·¯å¾‘æä¾›çµ¦æ¸¬è©¦å‡½å¼ï¼Œä¸¦åœ¨æ¸¬è©¦æœƒè©±çµæŸå¾Œè‡ªå‹•æ¸…ç†ç›®éŒ„ã€‚
    """
    # å»ºç«‹ä¸€å€‹å”¯ä¸€çš„è‡¨æ™‚ç›®éŒ„
    test_dir = tempfile.mkdtemp(prefix="test_evaluate_models_")
    
    # ä½¿ç”¨ yield å°‡æ§åˆ¶æ¬Šäº¤é‚„çµ¦æ¸¬è©¦å‡½å¼ï¼Œä¸¦æä¾› test_dir çš„å€¼
    yield test_dir
    
    # æ¸¬è©¦æœƒè©±çµæŸå¾Œï¼ŒåŸ·è¡Œæ¸…ç†å·¥ä½œ
    # shutil.rmtree() æœƒéè¿´åœ°åˆªé™¤æ•´å€‹ç›®éŒ„
    shutil.rmtree(test_dir, ignore_errors=True)


@pytest.fixture
def sample_config():
    """
    æä¾›ä¸€å€‹å›ºå®šçš„ã€ç”¨æ–¼æ¸¬è©¦çš„è¨­å®šå­—å…¸ã€‚
    é€™å€‹ fixture çš„é è¨­ scope æ˜¯ "function"ï¼Œè¡¨ç¤ºæ¯å€‹ä½¿ç”¨å®ƒçš„æ¸¬è©¦å‡½å¼éƒ½æœƒå¾—åˆ°ä¸€å€‹æ–°çš„å‰¯æœ¬ã€‚
    """
    # å‚³å›ä¸€å€‹åŒ…å«æ¨¡æ“¬è¨­å®šçš„å­—å…¸
    return {
        "OLLAMA_API_BASE_URL": "http://localhost:11434",
        "OLLAMA_MODELS_TO_COMPARE": ["test-model-1", "test-model-2"],
        "OPENAI_API_KEY": "test-openai-key",
        "GOOGLE_API_KEY": "test-google-key",
        "OPENROUTER_API_KEY": "test-openrouter-key",
        "REPLICATE_API_KEY": "test-replicate-key",
        "REVIEWER_MODELS": [
            {"provider": "openai", "model": "gpt-4"},
            {"provider": "gemini", "model": "gemini-pro"}
        ],
        "REVIEWER_TEMPERATURE": {
            "gpt-4": 0.1,
            "gemini-pro": 0.1,
            "o4-mini": 1,
            "gpt-4.1": 1,
        },
        "SUPPORTED_TASKS": {
            "translate": "è«‹å°‡ä»¥ä¸‹è‹±æ–‡ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ï¼š",
            "summarize": "è«‹ç‚ºä»¥ä¸‹å…§å®¹è£½ä½œç¹é«”ä¸­æ–‡æ‘˜è¦ï¼š"
        }
    }


@pytest.fixture
def sample_input_text():
    """
    æä¾›ä¸€æ®µå›ºå®šçš„ã€ç”¨æ–¼æ¸¬è©¦çš„è¼¸å…¥æ–‡å­—ã€‚
    """
    # å‚³å›ä¸€æ®µå¤šè¡Œå­—ä¸²ä½œç‚ºæ¸¬è©¦è¼¸å…¥
    return '''Hello everyone, thanks for joining today's meeting. 
We discussed the quarterly results and future plans. 
The team performed well this quarter with significant growth in revenue.
Key achievements include:
1. Launched new product features
2. Expanded to new markets
3. Improved customer satisfaction scores
Next quarter, we will focus on:
- Enhancing product quality
- Strengthening customer support
- Exploring partnership opportunities
Thank you for your attention.'''


@pytest.fixture
def mock_api_responses():
    """
    æä¾›ä¸€å€‹åŒ…å«æ¨¡æ“¬ API å›æ‡‰çš„å­—å…¸ã€‚
    ç”¨æ–¼åœ¨æ¸¬è©¦ä¸­å–ä»£å¯¦éš›çš„ API å‘¼å«ï¼Œé¿å…ç¶²è·¯å»¶é²å’Œå¤–éƒ¨ä¾è³´ã€‚
    """
    # å‚³å›ä¸€å€‹åŒ…å«å„ç¨®æ¨¡æ“¬ API å›æ‡‰çš„å­—å…¸
    return {
        "ollama_translate": "å¤§å®¶å¥½ï¼Œæ„Ÿè¬åƒåŠ ä»Šå¤©çš„æœƒè­°ã€‚æˆ‘å€‘è¨è«–äº†å­£åº¦çµæœå’Œæœªä¾†è¨ˆåŠƒã€‚",
        "ollama_summarize": "æœ¬æ¬¡æœƒè­°è¨è«–äº†å­£åº¦æ¥­ç¸¾å’Œæœªä¾†è¦åŠƒï¼Œåœ˜éšŠè¡¨ç¾è‰¯å¥½ã€‚",
        "openai_review": "åˆ†æ•¸: 8\nè©•èª: ç¿»è­¯æº–ç¢ºä¸”æµæš¢ï¼Œç¬¦åˆä¸­æ–‡è¡¨é”ç¿’æ…£ã€‚",
        "gemini_review": "åˆ†æ•¸: 7\nè©•èª: æ‘˜è¦é‡é»æ˜ç¢ºï¼Œä½†å¯ä»¥æ›´è©³ç´°ä¸€äº›ã€‚"
    }


@pytest.fixture
def temp_input_file(sample_input_text):
    """
    å»ºç«‹ä¸€å€‹åŒ…å«ç¯„ä¾‹æ–‡å­—çš„è‡¨æ™‚æª”æ¡ˆã€‚
    é€™å€‹ fixture ä¾è³´ `sample_input_text` fixtureã€‚
    
    ä½¿ç”¨ `with tempfile.NamedTemporaryFile` å¯ä»¥ç¢ºä¿æª”æ¡ˆåœ¨ä½¿ç”¨å¾Œè¢«å¦¥å–„è™•ç†ã€‚
    `delete=False` æ˜¯ç‚ºäº†åœ¨ `with` å€å¡ŠçµæŸå¾Œæª”æ¡ˆä»ç„¶å­˜åœ¨ï¼Œç›´åˆ° `yield` ä¹‹å¾Œæ‰‹å‹•åˆªé™¤ã€‚
    """
    # å»ºç«‹ä¸€å€‹å…·åçš„è‡¨æ™‚æª”æ¡ˆ
    with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
        # å°‡ç¯„ä¾‹æ–‡å­—å¯«å…¥æª”æ¡ˆ
        f.write(sample_input_text)
        # å–å¾—æª”æ¡ˆè·¯å¾‘
        temp_path = f.name
    
    # å°‡æª”æ¡ˆè·¯å¾‘æä¾›çµ¦æ¸¬è©¦å‡½å¼
    yield temp_path
    
    # æ¸¬è©¦å‡½å¼åŸ·è¡Œå®Œç•¢å¾Œï¼Œé€²è¡Œæ¸…ç†
    if os.path.exists(temp_path):
        os.unlink(temp_path)  # åˆªé™¤æª”æ¡ˆ


@pytest.fixture
def temp_cache_dir():
    """
    å»ºç«‹ä¸€å€‹è‡¨æ™‚ç›®éŒ„ï¼Œç”¨æ–¼æ¸¬è©¦å¿«å–åŠŸèƒ½ã€‚
    """
    # å»ºç«‹ä¸€å€‹å‰ç¶´ç‚º "test_cache_" çš„è‡¨æ™‚ç›®éŒ„
    temp_dir = tempfile.mkdtemp(prefix="test_cache_")
    
    # å°‡ç›®éŒ„è·¯å¾‘æä¾›çµ¦æ¸¬è©¦å‡½å¼
    yield temp_dir
    
    # æ¸¬è©¦çµæŸå¾Œï¼Œéè¿´åˆªé™¤ç›®éŒ„
    shutil.rmtree(temp_dir, ignore_errors=True)


@pytest.fixture
def mock_requests_post():
    """
    æ¨¡æ“¬ `requests.post` å‡½å¼ã€‚
    ä½¿ç”¨ `unittest.mock.patch` ä¾†å–ä»£ç›®æ¨™å‡½å¼ï¼Œä¸¦å‚³å›ä¸€å€‹ mock ç‰©ä»¶ã€‚
    """
    # ä½¿ç”¨ patch ä¾†æ¨¡æ“¬ 'requests.post'
    with patch('requests.post') as mock_post:
        # å°‡ mock ç‰©ä»¶æä¾›çµ¦æ¸¬è©¦å‡½å¼
        yield mock_post


@pytest.fixture
def mock_openai_client():
    """
    æ¨¡æ“¬ `openai.OpenAI` å®¢æˆ¶ç«¯ã€‚
    """
    # ä½¿ç”¨ patch ä¾†æ¨¡æ“¬ 'openai.OpenAI'
    with patch('openai.OpenAI') as mock_openai:
        # å°‡ mock ç‰©ä»¶æä¾›çµ¦æ¸¬è©¦å‡½å¼
        yield mock_openai


@pytest.fixture
def disable_cache():
    """
    åœ¨æ¸¬è©¦æœŸé–“åœç”¨å¿«å–åŠŸèƒ½ã€‚
    é€é patch `load_from_cache` å’Œ `save_to_cache` å‡½å¼ä¾†é”æˆã€‚
    `load_from_cache` æ°¸é å‚³å› Noneï¼Œæ¨¡æ“¬å¿«å–æœªå‘½ä¸­ã€‚
    `save_to_cache` ä¸åŸ·è¡Œä»»ä½•æ“ä½œã€‚
    """
    with patch('cache_utils.load_from_cache', return_value=None), \
         patch('cache_utils.save_to_cache'):
        yield


@pytest.fixture(autouse=True)
def setup_test_environment():
    """
    ç‚ºæ¯å€‹æ¸¬è©¦è‡ªå‹•è¨­å®šç’°å¢ƒã€‚
    `autouse=True` è¡¨ç¤ºé€™å€‹ fixture æœƒè‡ªå‹•è¢«æ‰€æœ‰æ¸¬è©¦ä½¿ç”¨ï¼Œç„¡éœ€æ‰‹å‹•æŒ‡å®šã€‚
    """
    # --- è¨­å®šéšæ®µ ---
    
    # è¨­å®šä¸€å€‹ç’°å¢ƒè®Šæ•¸ï¼Œç”¨ä¾†è­˜åˆ¥ç›®å‰æ˜¯å¦åœ¨æ¸¬è©¦ç’°å¢ƒä¸­
    os.environ['TESTING'] = '1'
    
    # ç¢ºä¿ reports ç›®éŒ„å­˜åœ¨ï¼Œä»¥ä¾¿æ¸¬è©¦å¯ä»¥å¯«å…¥å ±å‘Šæª”æ¡ˆ
    reports_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'reports')
    os.makedirs(reports_dir, exist_ok=True)
    
    # å°‡æ§åˆ¶æ¬Šäº¤çµ¦æ¸¬è©¦å‡½å¼
    yield
    
    # --- æ¸…ç†éšæ®µ ---
    
    # æ¸¬è©¦çµæŸå¾Œï¼Œåˆªé™¤è¨­å®šçš„ç’°å¢ƒè®Šæ•¸
    if 'TESTING' in os.environ:
        del os.environ['TESTING']


# --- Pytest Hooks: è‡ªè¨‚ pytest è¡Œç‚º ---

def pytest_configure(config):
    """
    åœ¨ pytest å•Ÿå‹•æ™‚è¨­å®šè‡ªè¨‚æ¨™è¨˜ (markers)ã€‚
    é€™å…è¨±æˆ‘å€‘ç”¨ `@pytest.mark.slow` ç­‰æ–¹å¼æ¨™è¨˜æ¸¬è©¦ã€‚
    """
    # æ–°å¢ 'slow' æ¨™è¨˜ï¼Œç”¨æ–¼æ¨™ç¤ºåŸ·è¡Œæ™‚é–“è¼ƒé•·çš„æ¸¬è©¦
    config.addinivalue_line("markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')")
    # æ–°å¢ 'integration' æ¨™è¨˜ï¼Œç”¨æ–¼æ¨™ç¤ºæ•´åˆæ¸¬è©¦
    config.addinivalue_line("markers", "integration: marks tests as integration tests")
    # æ–°å¢ 'unit' æ¨™è¨˜ï¼Œç”¨æ–¼æ¨™ç¤ºå–®å…ƒæ¸¬è©¦
    config.addinivalue_line("markers", "unit: marks tests as unit tests")
    # æ–°å¢ 'api' æ¨™è¨˜ï¼Œç”¨æ–¼æ¨™ç¤ºéœ€è¦å­˜å–å¤–éƒ¨ API çš„æ¸¬è©¦
    config.addinivalue_line("markers", "api: marks tests that require API access")


def pytest_collection_modifyitems(config, items):
    """
    åœ¨æ¸¬è©¦æ”¶é›†å®Œæˆå¾Œï¼Œä¿®æ”¹æ¸¬è©¦é …ç›®åˆ—è¡¨ã€‚
    é€™å€‹ hook ç”¨æ–¼ç‚ºæ²’æœ‰ä»»ä½•è‡ªè¨‚æ¨™è¨˜çš„æ¸¬è©¦è‡ªå‹•åŠ ä¸Š 'unit' æ¨™è¨˜ã€‚
    """
    # éæ­·æ‰€æœ‰æ”¶é›†åˆ°çš„æ¸¬è©¦é …ç›®
    for item in items:
        # æª¢æŸ¥è©²é …ç›®æ˜¯å¦å·²ç¶“æœ‰ 'unit', 'integration', 'slow', 'api' ä¸­çš„ä»»ä½•ä¸€å€‹æ¨™è¨˜
        if not any(mark.name in ['unit', 'integration', 'slow', 'api'] for mark in item.iter_markers()):
            # å¦‚æœæ²’æœ‰ï¼Œå°±ç‚ºå®ƒåŠ ä¸Š 'unit' æ¨™è¨˜
            item.add_marker(pytest.mark.unit)


def pytest_runtest_makereport(item, call):
    """
    ç‚ºæ¸¬è©¦å ±å‘Šæ·»åŠ é¡å¤–è³‡è¨Šã€‚
    é€™å€‹ hook åœ¨æ¸¬è©¦çš„ setup, call, teardown ä¸‰å€‹éšæ®µéƒ½æœƒè¢«å‘¼å«ã€‚
    """
    # è™•ç† 'incremental' æ¨™è¨˜ï¼Œå¦‚æœä¸€å€‹æ¸¬è©¦å¤±æ•—ï¼Œå‰‡è·³éå¾ŒçºŒä¾è³´å®ƒçš„æ¸¬è©¦
    if "incremental" in item.keywords:
        if call.excinfo is not None:
            parent = item.parent
            parent._previousfailed = item
    
    # å°‡æ¸¬è©¦å‘¼å« (call) çš„çµæœå ±å‘Šé™„åŠ åˆ°æ¸¬è©¦é …ç›® (item) ä¸Š
    # é€™å°æ–¼åœ¨ fixture ä¸­åˆ¤æ–·æ¸¬è©¦æ˜¯å¦å¤±æ•—å¾ˆæœ‰ç”¨
    if call.when == "call":
        setattr(item, "rep_" + call.when, call)


@pytest.fixture(autouse=True)
def cleanup_on_failure(request):
    """
    ä¸€å€‹ autouse fixtureï¼Œç”¨æ–¼åœ¨æ¸¬è©¦å¤±æ•—æ™‚åŸ·è¡Œæ¸…ç†å·¥ä½œã€‚
    """
    # å…ˆåŸ·è¡Œæ¸¬è©¦
    yield
    
    # æ¸¬è©¦çµæŸå¾Œï¼Œæª¢æŸ¥æ¸¬è©¦çµæœ
    # `request.node` ä»£è¡¨ç›®å‰çš„æ¸¬è©¦é …ç›®
    if hasattr(request.node, 'rep_call') and hasattr(request.node.rep_call, 'failed') and request.node.rep_call.failed:
        # å¦‚æœæ¸¬è©¦å¤±æ•—ï¼Œå¯ä»¥åœ¨é€™è£¡åŠ å…¥ç‰¹å®šçš„æ¸…ç†é‚è¼¯
        # ä¾‹å¦‚ï¼šå„²å­˜è¢å¹•æˆªåœ–ã€è¨˜éŒ„é¡å¤–æ—¥èªŒç­‰
        pass


def pytest_runtest_setup(item):
    """
    åœ¨æ¯å€‹æ¸¬è©¦çš„ setup éšæ®µåŸ·è¡Œã€‚
    ç”¨æ–¼å¯¦ç¾ 'incremental' æ¨™è¨˜çš„åŠŸèƒ½ã€‚
    """
    # å¦‚æœæ¸¬è©¦è¢«æ¨™è¨˜ç‚º 'incremental'
    if "incremental" in item.keywords:
        # æª¢æŸ¥æ˜¯å¦æœ‰å‰ä¸€å€‹æ¸¬è©¦å¤±æ•—çš„è¨˜éŒ„
        previousfailed = getattr(item.parent, "_previousfailed", None)
        if previousfailed is not None:
            # å¦‚æœæœ‰ï¼Œå‰‡ä½¿ç”¨ pytest.xfail ä¾†æ¨™è¨˜ç›®å‰æ¸¬è©¦ç‚ºé æœŸå¤±æ•—ï¼Œä¸¦æä¾›åŸå› 
            pytest.xfail("previous test failed (%s)" % previousfailed.name)


def pytest_sessionstart(session):
    """
    åœ¨æ•´å€‹æ¸¬è©¦æœƒè©± (session) é–‹å§‹æ™‚åŸ·è¡Œä¸€æ¬¡ã€‚
    """
    # å°å‡ºé–‹å§‹è¨Šæ¯
    print("\nğŸš€ é–‹å§‹åŸ·è¡Œ EvaluateModels å°ˆæ¡ˆæ¸¬è©¦...")


def pytest_sessionfinish(session, exitstatus):
    """
    åœ¨æ•´å€‹æ¸¬è©¦æœƒè©± (session) çµæŸæ™‚åŸ·è¡Œä¸€æ¬¡ã€‚
    """
    # æ ¹æ“šçµæŸç‹€æ…‹ç¢¼å°å‡ºä¸åŒçš„è¨Šæ¯
    if exitstatus == 0:
        print("\nâœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼")
    else:
        print(f"\nâŒ æ¸¬è©¦å¤±æ•—ï¼Œé€€å‡ºä»£ç¢¼: {exitstatus}") 